{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The underlying purpose of this exercise is to first design a neural network without the use of callbacks, which will lead to intentional overfitting to the training data initially. Then with the use of EarlyStopping and Dropouts we will see the benefit of such techniques in preventing overfitting and of getting a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data file.\n",
    "df = pd.read_csv('cancer_classification.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 569 entries, 0 to 568\n",
      "Data columns (total 31 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   mean radius              569 non-null    float64\n",
      " 1   mean texture             569 non-null    float64\n",
      " 2   mean perimeter           569 non-null    float64\n",
      " 3   mean area                569 non-null    float64\n",
      " 4   mean smoothness          569 non-null    float64\n",
      " 5   mean compactness         569 non-null    float64\n",
      " 6   mean concavity           569 non-null    float64\n",
      " 7   mean concave points      569 non-null    float64\n",
      " 8   mean symmetry            569 non-null    float64\n",
      " 9   mean fractal dimension   569 non-null    float64\n",
      " 10  radius error             569 non-null    float64\n",
      " 11  texture error            569 non-null    float64\n",
      " 12  perimeter error          569 non-null    float64\n",
      " 13  area error               569 non-null    float64\n",
      " 14  smoothness error         569 non-null    float64\n",
      " 15  compactness error        569 non-null    float64\n",
      " 16  concavity error          569 non-null    float64\n",
      " 17  concave points error     569 non-null    float64\n",
      " 18  symmetry error           569 non-null    float64\n",
      " 19  fractal dimension error  569 non-null    float64\n",
      " 20  worst radius             569 non-null    float64\n",
      " 21  worst texture            569 non-null    float64\n",
      " 22  worst perimeter          569 non-null    float64\n",
      " 23  worst area               569 non-null    float64\n",
      " 24  worst smoothness         569 non-null    float64\n",
      " 25  worst compactness        569 non-null    float64\n",
      " 26  worst concavity          569 non-null    float64\n",
      " 27  worst concave points     569 non-null    float64\n",
      " 28  worst symmetry           569 non-null    float64\n",
      " 29  worst fractal dimension  569 non-null    float64\n",
      " 30  benign_0__mal_1          569 non-null    int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 137.9 KB\n"
     ]
    }
   ],
   "source": [
    "# Get an early look at the data and perform basic analysis.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean radius</th>\n",
       "      <td>569.0</td>\n",
       "      <td>14.127292</td>\n",
       "      <td>3.524049</td>\n",
       "      <td>6.981000</td>\n",
       "      <td>11.700000</td>\n",
       "      <td>13.370000</td>\n",
       "      <td>15.780000</td>\n",
       "      <td>28.11000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean texture</th>\n",
       "      <td>569.0</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>39.28000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean perimeter</th>\n",
       "      <td>569.0</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>188.50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean area</th>\n",
       "      <td>569.0</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>2501.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean smoothness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.16340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean compactness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.34540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concavity</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.42680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concave points</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.20120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean symmetry</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.30400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>0.09744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.405172</td>\n",
       "      <td>0.277313</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.232400</td>\n",
       "      <td>0.324200</td>\n",
       "      <td>0.478900</td>\n",
       "      <td>2.87300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>1.216853</td>\n",
       "      <td>0.551648</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.833900</td>\n",
       "      <td>1.108000</td>\n",
       "      <td>1.474000</td>\n",
       "      <td>4.88500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>2.866059</td>\n",
       "      <td>2.021855</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>1.606000</td>\n",
       "      <td>2.287000</td>\n",
       "      <td>3.357000</td>\n",
       "      <td>21.98000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>40.337079</td>\n",
       "      <td>45.491006</td>\n",
       "      <td>6.802000</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>24.530000</td>\n",
       "      <td>45.190000</td>\n",
       "      <td>542.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.007041</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.005169</td>\n",
       "      <td>0.006380</td>\n",
       "      <td>0.008146</td>\n",
       "      <td>0.03113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.025478</td>\n",
       "      <td>0.017908</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.032450</td>\n",
       "      <td>0.13540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.031894</td>\n",
       "      <td>0.030186</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015090</td>\n",
       "      <td>0.025890</td>\n",
       "      <td>0.042050</td>\n",
       "      <td>0.39600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.011796</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.010930</td>\n",
       "      <td>0.014710</td>\n",
       "      <td>0.05279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.020542</td>\n",
       "      <td>0.008266</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.018730</td>\n",
       "      <td>0.023480</td>\n",
       "      <td>0.07895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal dimension error</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.003795</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.003187</td>\n",
       "      <td>0.004558</td>\n",
       "      <td>0.02984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst radius</th>\n",
       "      <td>569.0</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>36.04000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst texture</th>\n",
       "      <td>569.0</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>49.54000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst perimeter</th>\n",
       "      <td>569.0</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>251.20000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst area</th>\n",
       "      <td>569.0</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>4254.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst smoothness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.22260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst compactness</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>1.05800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concavity</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>1.25200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concave points</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.29100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst symmetry</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.66380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.083946</td>\n",
       "      <td>0.018061</td>\n",
       "      <td>0.055040</td>\n",
       "      <td>0.071460</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>0.20750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>benign_0__mal_1</th>\n",
       "      <td>569.0</td>\n",
       "      <td>0.627417</td>\n",
       "      <td>0.483918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         count        mean         std         min  \\\n",
       "mean radius              569.0   14.127292    3.524049    6.981000   \n",
       "mean texture             569.0   19.289649    4.301036    9.710000   \n",
       "mean perimeter           569.0   91.969033   24.298981   43.790000   \n",
       "mean area                569.0  654.889104  351.914129  143.500000   \n",
       "mean smoothness          569.0    0.096360    0.014064    0.052630   \n",
       "mean compactness         569.0    0.104341    0.052813    0.019380   \n",
       "mean concavity           569.0    0.088799    0.079720    0.000000   \n",
       "mean concave points      569.0    0.048919    0.038803    0.000000   \n",
       "mean symmetry            569.0    0.181162    0.027414    0.106000   \n",
       "mean fractal dimension   569.0    0.062798    0.007060    0.049960   \n",
       "radius error             569.0    0.405172    0.277313    0.111500   \n",
       "texture error            569.0    1.216853    0.551648    0.360200   \n",
       "perimeter error          569.0    2.866059    2.021855    0.757000   \n",
       "area error               569.0   40.337079   45.491006    6.802000   \n",
       "smoothness error         569.0    0.007041    0.003003    0.001713   \n",
       "compactness error        569.0    0.025478    0.017908    0.002252   \n",
       "concavity error          569.0    0.031894    0.030186    0.000000   \n",
       "concave points error     569.0    0.011796    0.006170    0.000000   \n",
       "symmetry error           569.0    0.020542    0.008266    0.007882   \n",
       "fractal dimension error  569.0    0.003795    0.002646    0.000895   \n",
       "worst radius             569.0   16.269190    4.833242    7.930000   \n",
       "worst texture            569.0   25.677223    6.146258   12.020000   \n",
       "worst perimeter          569.0  107.261213   33.602542   50.410000   \n",
       "worst area               569.0  880.583128  569.356993  185.200000   \n",
       "worst smoothness         569.0    0.132369    0.022832    0.071170   \n",
       "worst compactness        569.0    0.254265    0.157336    0.027290   \n",
       "worst concavity          569.0    0.272188    0.208624    0.000000   \n",
       "worst concave points     569.0    0.114606    0.065732    0.000000   \n",
       "worst symmetry           569.0    0.290076    0.061867    0.156500   \n",
       "worst fractal dimension  569.0    0.083946    0.018061    0.055040   \n",
       "benign_0__mal_1          569.0    0.627417    0.483918    0.000000   \n",
       "\n",
       "                                25%         50%          75%         max  \n",
       "mean radius               11.700000   13.370000    15.780000    28.11000  \n",
       "mean texture              16.170000   18.840000    21.800000    39.28000  \n",
       "mean perimeter            75.170000   86.240000   104.100000   188.50000  \n",
       "mean area                420.300000  551.100000   782.700000  2501.00000  \n",
       "mean smoothness            0.086370    0.095870     0.105300     0.16340  \n",
       "mean compactness           0.064920    0.092630     0.130400     0.34540  \n",
       "mean concavity             0.029560    0.061540     0.130700     0.42680  \n",
       "mean concave points        0.020310    0.033500     0.074000     0.20120  \n",
       "mean symmetry              0.161900    0.179200     0.195700     0.30400  \n",
       "mean fractal dimension     0.057700    0.061540     0.066120     0.09744  \n",
       "radius error               0.232400    0.324200     0.478900     2.87300  \n",
       "texture error              0.833900    1.108000     1.474000     4.88500  \n",
       "perimeter error            1.606000    2.287000     3.357000    21.98000  \n",
       "area error                17.850000   24.530000    45.190000   542.20000  \n",
       "smoothness error           0.005169    0.006380     0.008146     0.03113  \n",
       "compactness error          0.013080    0.020450     0.032450     0.13540  \n",
       "concavity error            0.015090    0.025890     0.042050     0.39600  \n",
       "concave points error       0.007638    0.010930     0.014710     0.05279  \n",
       "symmetry error             0.015160    0.018730     0.023480     0.07895  \n",
       "fractal dimension error    0.002248    0.003187     0.004558     0.02984  \n",
       "worst radius              13.010000   14.970000    18.790000    36.04000  \n",
       "worst texture             21.080000   25.410000    29.720000    49.54000  \n",
       "worst perimeter           84.110000   97.660000   125.400000   251.20000  \n",
       "worst area               515.300000  686.500000  1084.000000  4254.00000  \n",
       "worst smoothness           0.116600    0.131300     0.146000     0.22260  \n",
       "worst compactness          0.147200    0.211900     0.339100     1.05800  \n",
       "worst concavity            0.114500    0.226700     0.382900     1.25200  \n",
       "worst concave points       0.064930    0.099930     0.161400     0.29100  \n",
       "worst symmetry             0.250400    0.282200     0.317900     0.66380  \n",
       "worst fractal dimension    0.071460    0.080040     0.092080     0.20750  \n",
       "benign_0__mal_1            0.000000    1.000000     1.000000     1.00000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The label we will be trying to predict is 'benign_0_mal_1', this is a classification of a tumor being either benign or malignant in nature based on the features inputted.We will have a look at the count of either in the data set to ensure a fairly balanced occurance. We will also look at the correlations between features and the labe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20b51960408>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(x = 'benign_0__mal_1',data=df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Series.sort_values of mean radius               -0.730029\n",
       "mean texture              -0.415185\n",
       "mean perimeter            -0.742636\n",
       "mean area                 -0.708984\n",
       "mean smoothness           -0.358560\n",
       "mean compactness          -0.596534\n",
       "mean concavity            -0.696360\n",
       "mean concave points       -0.776614\n",
       "mean symmetry             -0.330499\n",
       "mean fractal dimension     0.012838\n",
       "radius error              -0.567134\n",
       "texture error              0.008303\n",
       "perimeter error           -0.556141\n",
       "area error                -0.548236\n",
       "smoothness error           0.067016\n",
       "compactness error         -0.292999\n",
       "concavity error           -0.253730\n",
       "concave points error      -0.408042\n",
       "symmetry error             0.006522\n",
       "fractal dimension error   -0.077972\n",
       "worst radius              -0.776454\n",
       "worst texture             -0.456903\n",
       "worst perimeter           -0.782914\n",
       "worst area                -0.733825\n",
       "worst smoothness          -0.421465\n",
       "worst compactness         -0.590998\n",
       "worst concavity           -0.659610\n",
       "worst concave points      -0.793566\n",
       "worst symmetry            -0.416294\n",
       "worst fractal dimension   -0.323872\n",
       "benign_0__mal_1            1.000000\n",
       "Name: benign_0__mal_1, dtype: float64>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.corr()['benign_0__mal_1'].sort_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up our data into train/test parts, creating our model, training it and evaluating it initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare and initialize X and y variables\n",
    "X = df.drop('benign_0__mal_1', axis=1).values\n",
    "y = df['benign_0__mal_1'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the train/test split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.25, random_state = 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data.\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\whoga\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/600\n",
      "426/426 [==============================] - 0s 735us/sample - loss: 0.6527 - val_loss: 0.6300\n",
      "Epoch 2/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.6142 - val_loss: 0.5978\n",
      "Epoch 3/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.5809 - val_loss: 0.5636\n",
      "Epoch 4/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.5424 - val_loss: 0.5202\n",
      "Epoch 5/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.5013 - val_loss: 0.4765\n",
      "Epoch 6/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.4624 - val_loss: 0.4387\n",
      "Epoch 7/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.4206 - val_loss: 0.4024\n",
      "Epoch 8/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.3895 - val_loss: 0.3648\n",
      "Epoch 9/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.3539 - val_loss: 0.3299\n",
      "Epoch 10/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.3237 - val_loss: 0.3016\n",
      "Epoch 11/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.2993 - val_loss: 0.2745\n",
      "Epoch 12/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.2763 - val_loss: 0.2572\n",
      "Epoch 13/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.2567 - val_loss: 0.2348\n",
      "Epoch 14/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.2392 - val_loss: 0.2206\n",
      "Epoch 15/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.2265 - val_loss: 0.2064\n",
      "Epoch 16/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.2126 - val_loss: 0.1963\n",
      "Epoch 17/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.2012 - val_loss: 0.1868\n",
      "Epoch 18/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1902 - val_loss: 0.1771\n",
      "Epoch 19/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1809 - val_loss: 0.1712\n",
      "Epoch 20/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.1724 - val_loss: 0.1640\n",
      "Epoch 21/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1677 - val_loss: 0.1581\n",
      "Epoch 22/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.1556 - val_loss: 0.1562\n",
      "Epoch 23/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1490 - val_loss: 0.1495\n",
      "Epoch 24/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.1415 - val_loss: 0.1439\n",
      "Epoch 25/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.1357 - val_loss: 0.1410\n",
      "Epoch 26/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1317 - val_loss: 0.1387\n",
      "Epoch 27/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.1274 - val_loss: 0.1342\n",
      "Epoch 28/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.1180 - val_loss: 0.1320\n",
      "Epoch 29/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.1135 - val_loss: 0.1297\n",
      "Epoch 30/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.1115 - val_loss: 0.1290\n",
      "Epoch 31/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1047 - val_loss: 0.1255\n",
      "Epoch 32/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.1005 - val_loss: 0.1280\n",
      "Epoch 33/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0975 - val_loss: 0.1218\n",
      "Epoch 34/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0958 - val_loss: 0.1211\n",
      "Epoch 35/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0905 - val_loss: 0.1233\n",
      "Epoch 36/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0883 - val_loss: 0.1203\n",
      "Epoch 37/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0859 - val_loss: 0.1188\n",
      "Epoch 38/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0826 - val_loss: 0.1193\n",
      "Epoch 39/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0810 - val_loss: 0.1182\n",
      "Epoch 40/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0785 - val_loss: 0.1172\n",
      "Epoch 41/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0796 - val_loss: 0.1142\n",
      "Epoch 42/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.0776 - val_loss: 0.1202\n",
      "Epoch 43/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0745 - val_loss: 0.1166\n",
      "Epoch 44/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0735 - val_loss: 0.1184\n",
      "Epoch 45/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0714 - val_loss: 0.1163\n",
      "Epoch 46/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0692 - val_loss: 0.1144\n",
      "Epoch 47/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0700 - val_loss: 0.1183\n",
      "Epoch 48/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0664 - val_loss: 0.1125\n",
      "Epoch 49/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0668 - val_loss: 0.1145\n",
      "Epoch 50/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0651 - val_loss: 0.1146\n",
      "Epoch 51/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0638 - val_loss: 0.1155\n",
      "Epoch 52/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0643 - val_loss: 0.1127\n",
      "Epoch 53/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0629 - val_loss: 0.1133\n",
      "Epoch 54/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0626 - val_loss: 0.1172\n",
      "Epoch 55/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0626 - val_loss: 0.1134\n",
      "Epoch 56/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0615 - val_loss: 0.1157\n",
      "Epoch 57/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0604 - val_loss: 0.1163\n",
      "Epoch 58/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0591 - val_loss: 0.1130\n",
      "Epoch 59/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0592 - val_loss: 0.1170\n",
      "Epoch 60/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0594 - val_loss: 0.1171\n",
      "Epoch 61/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0613 - val_loss: 0.1167\n",
      "Epoch 62/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0578 - val_loss: 0.1148\n",
      "Epoch 63/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0606 - val_loss: 0.1191\n",
      "Epoch 64/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0562 - val_loss: 0.1131\n",
      "Epoch 65/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0567 - val_loss: 0.1158\n",
      "Epoch 66/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0571 - val_loss: 0.1135\n",
      "Epoch 67/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0543 - val_loss: 0.1166\n",
      "Epoch 68/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0554 - val_loss: 0.1119\n",
      "Epoch 69/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0558 - val_loss: 0.1170\n",
      "Epoch 70/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0559 - val_loss: 0.1164\n",
      "Epoch 71/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0550 - val_loss: 0.1169\n",
      "Epoch 72/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0537 - val_loss: 0.1145\n",
      "Epoch 73/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0544 - val_loss: 0.1148\n",
      "Epoch 74/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0532 - val_loss: 0.1189\n",
      "Epoch 75/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0519 - val_loss: 0.1153\n",
      "Epoch 76/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0529 - val_loss: 0.1186\n",
      "Epoch 77/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0546 - val_loss: 0.1165\n",
      "Epoch 78/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0533 - val_loss: 0.1145\n",
      "Epoch 79/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0534 - val_loss: 0.1166\n",
      "Epoch 80/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0500 - val_loss: 0.1212\n",
      "Epoch 81/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0521 - val_loss: 0.1171\n",
      "Epoch 82/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0514 - val_loss: 0.1197\n",
      "Epoch 83/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0539 - val_loss: 0.1219\n",
      "Epoch 84/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0511 - val_loss: 0.1193\n",
      "Epoch 85/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0501 - val_loss: 0.1215\n",
      "Epoch 86/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0493 - val_loss: 0.1193\n",
      "Epoch 87/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0482 - val_loss: 0.1226\n",
      "Epoch 88/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0486 - val_loss: 0.1170\n",
      "Epoch 89/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0512 - val_loss: 0.1296\n",
      "Epoch 90/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0518 - val_loss: 0.1169\n",
      "Epoch 91/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0496 - val_loss: 0.1220\n",
      "Epoch 92/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0514 - val_loss: 0.1256\n",
      "Epoch 93/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0488 - val_loss: 0.1182\n",
      "Epoch 94/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0529 - val_loss: 0.1225\n",
      "Epoch 95/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0499 - val_loss: 0.1220\n",
      "Epoch 96/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0469 - val_loss: 0.1202\n",
      "Epoch 97/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0534 - val_loss: 0.1168\n",
      "Epoch 98/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0484 - val_loss: 0.1249\n",
      "Epoch 99/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0455 - val_loss: 0.1191\n",
      "Epoch 100/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0455 - val_loss: 0.1268\n",
      "Epoch 101/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0460 - val_loss: 0.1210\n",
      "Epoch 102/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0467 - val_loss: 0.1248\n",
      "Epoch 103/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0454 - val_loss: 0.1261\n",
      "Epoch 104/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0456 - val_loss: 0.1268\n",
      "Epoch 105/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0449 - val_loss: 0.1257\n",
      "Epoch 106/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0475 - val_loss: 0.1271\n",
      "Epoch 107/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0439 - val_loss: 0.1238\n",
      "Epoch 108/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0481 - val_loss: 0.1272\n",
      "Epoch 109/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0476 - val_loss: 0.1294\n",
      "Epoch 110/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0462 - val_loss: 0.1345\n",
      "Epoch 111/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0440 - val_loss: 0.1257\n",
      "Epoch 112/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0442 - val_loss: 0.1285\n",
      "Epoch 113/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0431 - val_loss: 0.1297\n",
      "Epoch 114/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0436 - val_loss: 0.1303\n",
      "Epoch 115/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0429 - val_loss: 0.1332\n",
      "Epoch 116/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0430 - val_loss: 0.1284\n",
      "Epoch 117/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0429 - val_loss: 0.1360\n",
      "Epoch 118/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0421 - val_loss: 0.1276\n",
      "Epoch 119/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0436 - val_loss: 0.1305\n",
      "Epoch 120/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0429 - val_loss: 0.1316\n",
      "Epoch 121/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0431 - val_loss: 0.1320\n",
      "Epoch 122/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0424 - val_loss: 0.1358\n",
      "Epoch 123/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0418 - val_loss: 0.1343\n",
      "Epoch 124/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0425 - val_loss: 0.1342\n",
      "Epoch 125/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0408 - val_loss: 0.1427\n",
      "Epoch 126/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0423 - val_loss: 0.1317\n",
      "Epoch 127/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0446 - val_loss: 0.1399\n",
      "Epoch 128/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0531 - val_loss: 0.1326\n",
      "Epoch 129/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0485 - val_loss: 0.1475\n",
      "Epoch 130/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0448 - val_loss: 0.1372\n",
      "Epoch 131/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0431 - val_loss: 0.1371\n",
      "Epoch 132/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0404 - val_loss: 0.1379\n",
      "Epoch 133/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0403 - val_loss: 0.1402\n",
      "Epoch 134/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0416 - val_loss: 0.1390\n",
      "Epoch 135/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0406 - val_loss: 0.1398\n",
      "Epoch 136/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0408 - val_loss: 0.1454\n",
      "Epoch 137/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0435 - val_loss: 0.1487\n",
      "Epoch 138/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0416 - val_loss: 0.1389\n",
      "Epoch 139/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0417 - val_loss: 0.1420\n",
      "Epoch 140/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0407 - val_loss: 0.1382\n",
      "Epoch 141/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0410 - val_loss: 0.1384\n",
      "Epoch 142/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0408 - val_loss: 0.1464\n",
      "Epoch 143/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0403 - val_loss: 0.1365\n",
      "Epoch 144/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0396 - val_loss: 0.1431\n",
      "Epoch 145/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0402 - val_loss: 0.1420\n",
      "Epoch 146/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0400 - val_loss: 0.1380\n",
      "Epoch 147/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0394 - val_loss: 0.1458\n",
      "Epoch 148/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0398 - val_loss: 0.1448\n",
      "Epoch 149/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0396 - val_loss: 0.1420\n",
      "Epoch 150/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0458 - val_loss: 0.1386\n",
      "Epoch 151/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0405 - val_loss: 0.1501\n",
      "Epoch 152/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0394 - val_loss: 0.1466\n",
      "Epoch 153/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0413 - val_loss: 0.1424\n",
      "Epoch 154/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0497 - val_loss: 0.1448\n",
      "Epoch 155/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0400 - val_loss: 0.1380\n",
      "Epoch 156/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0378 - val_loss: 0.1495\n",
      "Epoch 157/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0385 - val_loss: 0.1438\n",
      "Epoch 158/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0393 - val_loss: 0.1416\n",
      "Epoch 159/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0374 - val_loss: 0.1428\n",
      "Epoch 160/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0380 - val_loss: 0.1445\n",
      "Epoch 161/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0430 - val_loss: 0.1451\n",
      "Epoch 162/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0393 - val_loss: 0.1452\n",
      "Epoch 163/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0366 - val_loss: 0.1533\n",
      "Epoch 164/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0367 - val_loss: 0.1446\n",
      "Epoch 165/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0364 - val_loss: 0.1468\n",
      "Epoch 166/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0362 - val_loss: 0.1517\n",
      "Epoch 167/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0361 - val_loss: 0.1481\n",
      "Epoch 168/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0368 - val_loss: 0.1564\n",
      "Epoch 169/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0398 - val_loss: 0.1525\n",
      "Epoch 170/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0359 - val_loss: 0.1470\n",
      "Epoch 171/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0370 - val_loss: 0.1512\n",
      "Epoch 172/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0369 - val_loss: 0.1539\n",
      "Epoch 173/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0397 - val_loss: 0.1460\n",
      "Epoch 174/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0415 - val_loss: 0.1572\n",
      "Epoch 175/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0397 - val_loss: 0.1502\n",
      "Epoch 176/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0534 - val_loss: 0.1466\n",
      "Epoch 177/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0355 - val_loss: 0.1615\n",
      "Epoch 178/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0355 - val_loss: 0.1496\n",
      "Epoch 179/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0351 - val_loss: 0.1441\n",
      "Epoch 180/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0381 - val_loss: 0.1672\n",
      "Epoch 181/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0359 - val_loss: 0.1436\n",
      "Epoch 182/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0451 - val_loss: 0.1674\n",
      "Epoch 183/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0377 - val_loss: 0.1484\n",
      "Epoch 184/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0365 - val_loss: 0.1521\n",
      "Epoch 185/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0343 - val_loss: 0.1556\n",
      "Epoch 186/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0351 - val_loss: 0.1516\n",
      "Epoch 187/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0384 - val_loss: 0.1538\n",
      "Epoch 188/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0394 - val_loss: 0.1509\n",
      "Epoch 189/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0345 - val_loss: 0.1502\n",
      "Epoch 190/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0363 - val_loss: 0.1606\n",
      "Epoch 191/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0371 - val_loss: 0.1543\n",
      "Epoch 192/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0378 - val_loss: 0.1571\n",
      "Epoch 193/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0340 - val_loss: 0.1558\n",
      "Epoch 194/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0344 - val_loss: 0.1493\n",
      "Epoch 195/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0343 - val_loss: 0.1732\n",
      "Epoch 196/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0396 - val_loss: 0.1503\n",
      "Epoch 197/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0393 - val_loss: 0.1642\n",
      "Epoch 198/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0413 - val_loss: 0.1528\n",
      "Epoch 199/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0388 - val_loss: 0.1486\n",
      "Epoch 200/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0345 - val_loss: 0.1608\n",
      "Epoch 201/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0369 - val_loss: 0.1526\n",
      "Epoch 202/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0358 - val_loss: 0.1594\n",
      "Epoch 203/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0343 - val_loss: 0.1566\n",
      "Epoch 204/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0337 - val_loss: 0.1590\n",
      "Epoch 205/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0336 - val_loss: 0.1613\n",
      "Epoch 206/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0329 - val_loss: 0.1678\n",
      "Epoch 207/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0370 - val_loss: 0.1542\n",
      "Epoch 208/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0384 - val_loss: 0.1724\n",
      "Epoch 209/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0355 - val_loss: 0.1569\n",
      "Epoch 210/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0315 - val_loss: 0.1694\n",
      "Epoch 211/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0365 - val_loss: 0.1549\n",
      "Epoch 212/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0337 - val_loss: 0.1609\n",
      "Epoch 213/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0343 - val_loss: 0.1638\n",
      "Epoch 214/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0317 - val_loss: 0.1590\n",
      "Epoch 215/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0316 - val_loss: 0.1670\n",
      "Epoch 216/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0326 - val_loss: 0.1647\n",
      "Epoch 217/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0365 - val_loss: 0.1665\n",
      "Epoch 218/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0331 - val_loss: 0.1657\n",
      "Epoch 219/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0328 - val_loss: 0.1576\n",
      "Epoch 220/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0314 - val_loss: 0.1698\n",
      "Epoch 221/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0313 - val_loss: 0.1594\n",
      "Epoch 222/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0315 - val_loss: 0.1649\n",
      "Epoch 223/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0309 - val_loss: 0.1654\n",
      "Epoch 224/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0325 - val_loss: 0.1676\n",
      "Epoch 225/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0306 - val_loss: 0.1590\n",
      "Epoch 226/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0305 - val_loss: 0.1715\n",
      "Epoch 227/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0322 - val_loss: 0.1649\n",
      "Epoch 228/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0348 - val_loss: 0.1628\n",
      "Epoch 229/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0306 - val_loss: 0.1757\n",
      "Epoch 230/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0308 - val_loss: 0.1638\n",
      "Epoch 231/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0298 - val_loss: 0.1653\n",
      "Epoch 232/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0310 - val_loss: 0.1723\n",
      "Epoch 233/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0309 - val_loss: 0.1722\n",
      "Epoch 234/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0306 - val_loss: 0.1642\n",
      "Epoch 235/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0310 - val_loss: 0.1720\n",
      "Epoch 236/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0314 - val_loss: 0.1672\n",
      "Epoch 237/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0328 - val_loss: 0.1738\n",
      "Epoch 238/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0315 - val_loss: 0.1698\n",
      "Epoch 239/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0299 - val_loss: 0.1650\n",
      "Epoch 240/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0305 - val_loss: 0.1656\n",
      "Epoch 241/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0305 - val_loss: 0.1819\n",
      "Epoch 242/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0293 - val_loss: 0.1666\n",
      "Epoch 243/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0298 - val_loss: 0.1714\n",
      "Epoch 244/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0295 - val_loss: 0.1747\n",
      "Epoch 245/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0297 - val_loss: 0.1773\n",
      "Epoch 246/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0293 - val_loss: 0.1718\n",
      "Epoch 247/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0321 - val_loss: 0.1698\n",
      "Epoch 248/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0352 - val_loss: 0.1803\n",
      "Epoch 249/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0304 - val_loss: 0.1635\n",
      "Epoch 250/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0323 - val_loss: 0.1741\n",
      "Epoch 251/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0284 - val_loss: 0.1748\n",
      "Epoch 252/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0347 - val_loss: 0.1664\n",
      "Epoch 253/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0344 - val_loss: 0.1816\n",
      "Epoch 254/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0282 - val_loss: 0.1760\n",
      "Epoch 255/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0369 - val_loss: 0.1878\n",
      "Epoch 256/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0341 - val_loss: 0.1703\n",
      "Epoch 257/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0325 - val_loss: 0.1857\n",
      "Epoch 258/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0275 - val_loss: 0.1707\n",
      "Epoch 259/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0279 - val_loss: 0.1698\n",
      "Epoch 260/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0300 - val_loss: 0.1774\n",
      "Epoch 261/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0280 - val_loss: 0.1730\n",
      "Epoch 262/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0304 - val_loss: 0.1748\n",
      "Epoch 263/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0314 - val_loss: 0.1905\n",
      "Epoch 264/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0277 - val_loss: 0.1722\n",
      "Epoch 265/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0280 - val_loss: 0.1790\n",
      "Epoch 266/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0293 - val_loss: 0.1876\n",
      "Epoch 267/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0283 - val_loss: 0.1682\n",
      "Epoch 268/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0317 - val_loss: 0.1848\n",
      "Epoch 269/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0270 - val_loss: 0.1785\n",
      "Epoch 270/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0327 - val_loss: 0.1771\n",
      "Epoch 271/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0268 - val_loss: 0.1747\n",
      "Epoch 272/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0262 - val_loss: 0.1872\n",
      "Epoch 273/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0277 - val_loss: 0.1751\n",
      "Epoch 274/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0278 - val_loss: 0.1859\n",
      "Epoch 275/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0282 - val_loss: 0.1887\n",
      "Epoch 276/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0262 - val_loss: 0.1799\n",
      "Epoch 277/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0265 - val_loss: 0.1861\n",
      "Epoch 278/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0256 - val_loss: 0.1771\n",
      "Epoch 279/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0255 - val_loss: 0.1982\n",
      "Epoch 280/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0265 - val_loss: 0.1728\n",
      "Epoch 281/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0258 - val_loss: 0.1824\n",
      "Epoch 282/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0249 - val_loss: 0.1825\n",
      "Epoch 283/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0246 - val_loss: 0.1857\n",
      "Epoch 284/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0265 - val_loss: 0.1781\n",
      "Epoch 285/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0279 - val_loss: 0.1810\n",
      "Epoch 286/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0341 - val_loss: 0.2002\n",
      "Epoch 287/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0254 - val_loss: 0.1795\n",
      "Epoch 288/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0271 - val_loss: 0.1883\n",
      "Epoch 289/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0252 - val_loss: 0.1894\n",
      "Epoch 290/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0247 - val_loss: 0.1784\n",
      "Epoch 291/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0259 - val_loss: 0.1836\n",
      "Epoch 292/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0244 - val_loss: 0.1829\n",
      "Epoch 293/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0253 - val_loss: 0.1843\n",
      "Epoch 294/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0265 - val_loss: 0.1859\n",
      "Epoch 295/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0235 - val_loss: 0.1902\n",
      "Epoch 296/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0245 - val_loss: 0.1825\n",
      "Epoch 297/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0239 - val_loss: 0.2004\n",
      "Epoch 298/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0285 - val_loss: 0.1842\n",
      "Epoch 299/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0228 - val_loss: 0.2078\n",
      "Epoch 300/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0269 - val_loss: 0.1797\n",
      "Epoch 301/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0254 - val_loss: 0.2144\n",
      "Epoch 302/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0307 - val_loss: 0.1963\n",
      "Epoch 303/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0253 - val_loss: 0.1861\n",
      "Epoch 304/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0247 - val_loss: 0.2111\n",
      "Epoch 305/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0249 - val_loss: 0.1855\n",
      "Epoch 306/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0239 - val_loss: 0.1928\n",
      "Epoch 307/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0229 - val_loss: 0.2027\n",
      "Epoch 308/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0213 - val_loss: 0.1804\n",
      "Epoch 309/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0293 - val_loss: 0.2247\n",
      "Epoch 310/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0277 - val_loss: 0.1842\n",
      "Epoch 311/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0220 - val_loss: 0.1964\n",
      "Epoch 312/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0217 - val_loss: 0.1803\n",
      "Epoch 313/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0255 - val_loss: 0.2046\n",
      "Epoch 314/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0239 - val_loss: 0.1920\n",
      "Epoch 315/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0257 - val_loss: 0.1857\n",
      "Epoch 316/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0261 - val_loss: 0.2189\n",
      "Epoch 317/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0260 - val_loss: 0.1829\n",
      "Epoch 318/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0205 - val_loss: 0.2166\n",
      "Epoch 319/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0213 - val_loss: 0.1916\n",
      "Epoch 320/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0219 - val_loss: 0.2033\n",
      "Epoch 321/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0241 - val_loss: 0.1916\n",
      "Epoch 322/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0226 - val_loss: 0.1938\n",
      "Epoch 323/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0214 - val_loss: 0.1993\n",
      "Epoch 324/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0205 - val_loss: 0.2097\n",
      "Epoch 325/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0212 - val_loss: 0.1957\n",
      "Epoch 326/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0223 - val_loss: 0.2030\n",
      "Epoch 327/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0221 - val_loss: 0.1922\n",
      "Epoch 328/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0218 - val_loss: 0.2014\n",
      "Epoch 329/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0205 - val_loss: 0.2026\n",
      "Epoch 330/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0200 - val_loss: 0.1954\n",
      "Epoch 331/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0217 - val_loss: 0.2105\n",
      "Epoch 332/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0210 - val_loss: 0.2007\n",
      "Epoch 333/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0196 - val_loss: 0.2007\n",
      "Epoch 334/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0225 - val_loss: 0.2059\n",
      "Epoch 335/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0236 - val_loss: 0.2149\n",
      "Epoch 336/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0330 - val_loss: 0.1931\n",
      "Epoch 337/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0209 - val_loss: 0.2016\n",
      "Epoch 338/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0204 - val_loss: 0.1959\n",
      "Epoch 339/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0194 - val_loss: 0.1986\n",
      "Epoch 340/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0200 - val_loss: 0.2010\n",
      "Epoch 341/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0192 - val_loss: 0.1986\n",
      "Epoch 342/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0234 - val_loss: 0.2119\n",
      "Epoch 343/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0216 - val_loss: 0.2043\n",
      "Epoch 344/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0191 - val_loss: 0.2094\n",
      "Epoch 345/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0192 - val_loss: 0.1974\n",
      "Epoch 346/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0193 - val_loss: 0.2092\n",
      "Epoch 347/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0212 - val_loss: 0.2117\n",
      "Epoch 348/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0198 - val_loss: 0.2107\n",
      "Epoch 349/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0188 - val_loss: 0.2066\n",
      "Epoch 350/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0188 - val_loss: 0.2141\n",
      "Epoch 351/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0236 - val_loss: 0.2081\n",
      "Epoch 352/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0218 - val_loss: 0.2067\n",
      "Epoch 353/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0184 - val_loss: 0.1948\n",
      "Epoch 354/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0192 - val_loss: 0.2187\n",
      "Epoch 355/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0201 - val_loss: 0.2065\n",
      "Epoch 356/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0189 - val_loss: 0.2168\n",
      "Epoch 357/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0212 - val_loss: 0.2047\n",
      "Epoch 358/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0224 - val_loss: 0.2246\n",
      "Epoch 359/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0173 - val_loss: 0.2059\n",
      "Epoch 360/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0182 - val_loss: 0.2113\n",
      "Epoch 361/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0175 - val_loss: 0.2100\n",
      "Epoch 362/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0184 - val_loss: 0.2327\n",
      "Epoch 363/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0187 - val_loss: 0.2159\n",
      "Epoch 364/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0180 - val_loss: 0.2201\n",
      "Epoch 365/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0171 - val_loss: 0.2042\n",
      "Epoch 366/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0180 - val_loss: 0.2500\n",
      "Epoch 367/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0205 - val_loss: 0.2089\n",
      "Epoch 368/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0170 - val_loss: 0.2321\n",
      "Epoch 369/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0187 - val_loss: 0.2177\n",
      "Epoch 370/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0169 - val_loss: 0.2093\n",
      "Epoch 371/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0176 - val_loss: 0.2362\n",
      "Epoch 372/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0192 - val_loss: 0.2097\n",
      "Epoch 373/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0175 - val_loss: 0.2214\n",
      "Epoch 374/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0174 - val_loss: 0.2158\n",
      "Epoch 375/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0173 - val_loss: 0.2058\n",
      "Epoch 376/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0167 - val_loss: 0.2236\n",
      "Epoch 377/600\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.003 - 0s 33us/sample - loss: 0.0170 - val_loss: 0.2130\n",
      "Epoch 378/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0173 - val_loss: 0.2102\n",
      "Epoch 379/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0202 - val_loss: 0.2248\n",
      "Epoch 380/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0172 - val_loss: 0.2321\n",
      "Epoch 381/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0170 - val_loss: 0.2127\n",
      "Epoch 382/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0188 - val_loss: 0.2435\n",
      "Epoch 383/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0187 - val_loss: 0.2150\n",
      "Epoch 384/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0243 - val_loss: 0.2047\n",
      "Epoch 385/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0174 - val_loss: 0.2418\n",
      "Epoch 386/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0230 - val_loss: 0.2074\n",
      "Epoch 387/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0167 - val_loss: 0.2219\n",
      "Epoch 388/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0172 - val_loss: 0.2153\n",
      "Epoch 389/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0176 - val_loss: 0.2196\n",
      "Epoch 390/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0174 - val_loss: 0.2231\n",
      "Epoch 391/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0155 - val_loss: 0.2233\n",
      "Epoch 392/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0159 - val_loss: 0.2325\n",
      "Epoch 393/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0201 - val_loss: 0.2108\n",
      "Epoch 394/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0184 - val_loss: 0.2360\n",
      "Epoch 395/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0162 - val_loss: 0.2247\n",
      "Epoch 396/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0153 - val_loss: 0.2169\n",
      "Epoch 397/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0175 - val_loss: 0.2341\n",
      "Epoch 398/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0164 - val_loss: 0.2287\n",
      "Epoch 399/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0149 - val_loss: 0.2246\n",
      "Epoch 400/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0191 - val_loss: 0.2364\n",
      "Epoch 401/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0159 - val_loss: 0.2239\n",
      "Epoch 402/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0210 - val_loss: 0.2618\n",
      "Epoch 403/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0171 - val_loss: 0.2152\n",
      "Epoch 404/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0161 - val_loss: 0.2431\n",
      "Epoch 405/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0154 - val_loss: 0.2285\n",
      "Epoch 406/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0155 - val_loss: 0.2349\n",
      "Epoch 407/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0219 - val_loss: 0.2212\n",
      "Epoch 408/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0152 - val_loss: 0.2344\n",
      "Epoch 409/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0151 - val_loss: 0.2255\n",
      "Epoch 410/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0145 - val_loss: 0.2335\n",
      "Epoch 411/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0161 - val_loss: 0.2417\n",
      "Epoch 412/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0151 - val_loss: 0.2398\n",
      "Epoch 413/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0147 - val_loss: 0.2392\n",
      "Epoch 414/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0148 - val_loss: 0.2359\n",
      "Epoch 415/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0137 - val_loss: 0.2371\n",
      "Epoch 416/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0135 - val_loss: 0.2275\n",
      "Epoch 417/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0137 - val_loss: 0.2350\n",
      "Epoch 418/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0141 - val_loss: 0.2370\n",
      "Epoch 419/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0137 - val_loss: 0.2361\n",
      "Epoch 420/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0136 - val_loss: 0.2351\n",
      "Epoch 421/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0153 - val_loss: 0.2283\n",
      "Epoch 422/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0136 - val_loss: 0.2319\n",
      "Epoch 423/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0138 - val_loss: 0.2521\n",
      "Epoch 424/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0152 - val_loss: 0.2342\n",
      "Epoch 425/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0150 - val_loss: 0.2328\n",
      "Epoch 426/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0131 - val_loss: 0.2370\n",
      "Epoch 427/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0139 - val_loss: 0.2463\n",
      "Epoch 428/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0185 - val_loss: 0.2249\n",
      "Epoch 429/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0146 - val_loss: 0.2535\n",
      "Epoch 430/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0143 - val_loss: 0.2460\n",
      "Epoch 431/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0124 - val_loss: 0.2415\n",
      "Epoch 432/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0142 - val_loss: 0.2419\n",
      "Epoch 433/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0156 - val_loss: 0.2603\n",
      "Epoch 434/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0146 - val_loss: 0.2283\n",
      "Epoch 435/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0129 - val_loss: 0.2615\n",
      "Epoch 436/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0126 - val_loss: 0.2314\n",
      "Epoch 437/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0166 - val_loss: 0.2413\n",
      "Epoch 438/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0167 - val_loss: 0.2582\n",
      "Epoch 439/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0146 - val_loss: 0.2378\n",
      "Epoch 440/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0132 - val_loss: 0.2411\n",
      "Epoch 441/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0123 - val_loss: 0.2565\n",
      "Epoch 442/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0130 - val_loss: 0.2536\n",
      "Epoch 443/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0173 - val_loss: 0.2422\n",
      "Epoch 444/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0154 - val_loss: 0.2452\n",
      "Epoch 445/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0179 - val_loss: 0.2621\n",
      "Epoch 446/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0116 - val_loss: 0.2424\n",
      "Epoch 447/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0138 - val_loss: 0.2791\n",
      "Epoch 448/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0204 - val_loss: 0.2335\n",
      "Epoch 449/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0125 - val_loss: 0.2534\n",
      "Epoch 450/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0118 - val_loss: 0.2453\n",
      "Epoch 451/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0166 - val_loss: 0.2792\n",
      "Epoch 452/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0152 - val_loss: 0.2366\n",
      "Epoch 453/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0122 - val_loss: 0.2552\n",
      "Epoch 454/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0114 - val_loss: 0.2442\n",
      "Epoch 455/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0115 - val_loss: 0.2525\n",
      "Epoch 456/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0128 - val_loss: 0.2381\n",
      "Epoch 457/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0142 - val_loss: 0.2605\n",
      "Epoch 458/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0120 - val_loss: 0.2599\n",
      "Epoch 459/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0125 - val_loss: 0.2490\n",
      "Epoch 460/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0123 - val_loss: 0.2856\n",
      "Epoch 461/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0122 - val_loss: 0.2369\n",
      "Epoch 462/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0164 - val_loss: 0.3146\n",
      "Epoch 463/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0178 - val_loss: 0.2494\n",
      "Epoch 464/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0101 - val_loss: 0.2590\n",
      "Epoch 465/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0105 - val_loss: 0.2514\n",
      "Epoch 466/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0107 - val_loss: 0.2482\n",
      "Epoch 467/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0107 - val_loss: 0.2556\n",
      "Epoch 468/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0112 - val_loss: 0.2631\n",
      "Epoch 469/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0105 - val_loss: 0.2689\n",
      "Epoch 470/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0102 - val_loss: 0.2615\n",
      "Epoch 471/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0101 - val_loss: 0.2598\n",
      "Epoch 472/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0118 - val_loss: 0.2667\n",
      "Epoch 473/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0106 - val_loss: 0.2675\n",
      "Epoch 474/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0099 - val_loss: 0.2514\n",
      "Epoch 475/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0118 - val_loss: 0.2590\n",
      "Epoch 476/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0145 - val_loss: 0.3137\n",
      "Epoch 477/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0165 - val_loss: 0.2411\n",
      "Epoch 478/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0164 - val_loss: 0.2872\n",
      "Epoch 479/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0101 - val_loss: 0.2509\n",
      "Epoch 480/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0114 - val_loss: 0.2919\n",
      "Epoch 481/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0128 - val_loss: 0.2476\n",
      "Epoch 482/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0094 - val_loss: 0.2739\n",
      "Epoch 483/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0103 - val_loss: 0.2556\n",
      "Epoch 484/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0108 - val_loss: 0.2514\n",
      "Epoch 485/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0113 - val_loss: 0.2800\n",
      "Epoch 486/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0100 - val_loss: 0.2566\n",
      "Epoch 487/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0100 - val_loss: 0.2784\n",
      "Epoch 488/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0121 - val_loss: 0.2499\n",
      "Epoch 489/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0117 - val_loss: 0.2543\n",
      "Epoch 490/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0113 - val_loss: 0.2763\n",
      "Epoch 491/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0096 - val_loss: 0.2555\n",
      "Epoch 492/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0105 - val_loss: 0.2664\n",
      "Epoch 493/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0102 - val_loss: 0.2767\n",
      "Epoch 494/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0094 - val_loss: 0.2557\n",
      "Epoch 495/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0105 - val_loss: 0.2800\n",
      "Epoch 496/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0092 - val_loss: 0.2644\n",
      "Epoch 497/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0093 - val_loss: 0.2626\n",
      "Epoch 498/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0094 - val_loss: 0.2791\n",
      "Epoch 499/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0087 - val_loss: 0.2647\n",
      "Epoch 500/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0097 - val_loss: 0.2870\n",
      "Epoch 501/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0093 - val_loss: 0.2677\n",
      "Epoch 502/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0085 - val_loss: 0.2799\n",
      "Epoch 503/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0084 - val_loss: 0.2644\n",
      "Epoch 504/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0110 - val_loss: 0.2617\n",
      "Epoch 505/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0091 - val_loss: 0.2872\n",
      "Epoch 506/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0101 - val_loss: 0.2545\n",
      "Epoch 507/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0108 - val_loss: 0.2926\n",
      "Epoch 508/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0112 - val_loss: 0.2792\n",
      "Epoch 509/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0132 - val_loss: 0.2584\n",
      "Epoch 510/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0124 - val_loss: 0.2705\n",
      "Epoch 511/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0109 - val_loss: 0.3023\n",
      "Epoch 512/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0097 - val_loss: 0.2715\n",
      "Epoch 513/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0091 - val_loss: 0.2669\n",
      "Epoch 514/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0085 - val_loss: 0.2717\n",
      "Epoch 515/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0086 - val_loss: 0.2941\n",
      "Epoch 516/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0086 - val_loss: 0.2706\n",
      "Epoch 517/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0082 - val_loss: 0.2924\n",
      "Epoch 518/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0083 - val_loss: 0.2738\n",
      "Epoch 519/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0090 - val_loss: 0.2645\n",
      "Epoch 520/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0091 - val_loss: 0.3102\n",
      "Epoch 521/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0100 - val_loss: 0.2777\n",
      "Epoch 522/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0086 - val_loss: 0.2827\n",
      "Epoch 523/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0082 - val_loss: 0.2771\n",
      "Epoch 524/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0079 - val_loss: 0.2833\n",
      "Epoch 525/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0092 - val_loss: 0.2727\n",
      "Epoch 526/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0099 - val_loss: 0.2941\n",
      "Epoch 527/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0071 - val_loss: 0.2747\n",
      "Epoch 528/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0077 - val_loss: 0.2955\n",
      "Epoch 529/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0074 - val_loss: 0.2809\n",
      "Epoch 530/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0074 - val_loss: 0.2871\n",
      "Epoch 531/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0085 - val_loss: 0.2738\n",
      "Epoch 532/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0144 - val_loss: 0.2948\n",
      "Epoch 533/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0102 - val_loss: 0.2967\n",
      "Epoch 534/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0097 - val_loss: 0.2950\n",
      "Epoch 535/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0088 - val_loss: 0.2766\n",
      "Epoch 536/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0076 - val_loss: 0.2840\n",
      "Epoch 537/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0072 - val_loss: 0.2873\n",
      "Epoch 538/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0072 - val_loss: 0.3033\n",
      "Epoch 539/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0078 - val_loss: 0.2835\n",
      "Epoch 540/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0076 - val_loss: 0.3155\n",
      "Epoch 541/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0090 - val_loss: 0.2806\n",
      "Epoch 542/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0086 - val_loss: 0.2988\n",
      "Epoch 543/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0076 - val_loss: 0.3168\n",
      "Epoch 544/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0092 - val_loss: 0.2778\n",
      "Epoch 545/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0085 - val_loss: 0.3238\n",
      "Epoch 546/600\n",
      "426/426 [==============================] - ETA: 0s - loss: 0.001 - 0s 33us/sample - loss: 0.0068 - val_loss: 0.2799\n",
      "Epoch 547/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0077 - val_loss: 0.3501\n",
      "Epoch 548/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0132 - val_loss: 0.2738\n",
      "Epoch 549/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0130 - val_loss: 0.3167\n",
      "Epoch 550/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0115 - val_loss: 0.3291\n",
      "Epoch 551/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0099 - val_loss: 0.2713\n",
      "Epoch 552/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0149 - val_loss: 0.3554\n",
      "Epoch 553/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0100 - val_loss: 0.2873\n",
      "Epoch 554/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0076 - val_loss: 0.3151\n",
      "Epoch 555/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0072 - val_loss: 0.2984\n",
      "Epoch 556/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0068 - val_loss: 0.2915\n",
      "Epoch 557/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0071 - val_loss: 0.3052\n",
      "Epoch 558/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0065 - val_loss: 0.2941\n",
      "Epoch 559/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0071 - val_loss: 0.3229\n",
      "Epoch 560/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0083 - val_loss: 0.2888\n",
      "Epoch 561/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0062 - val_loss: 0.3120\n",
      "Epoch 562/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0077 - val_loss: 0.2992\n",
      "Epoch 563/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0069 - val_loss: 0.2963\n",
      "Epoch 564/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0065 - val_loss: 0.2937\n",
      "Epoch 565/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0060 - val_loss: 0.3066\n",
      "Epoch 566/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0084 - val_loss: 0.2955\n",
      "Epoch 567/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0063 - val_loss: 0.3154\n",
      "Epoch 568/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0054 - val_loss: 0.2834\n",
      "Epoch 569/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0090 - val_loss: 0.3141\n",
      "Epoch 570/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0093 - val_loss: 0.3150\n",
      "Epoch 571/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0078 - val_loss: 0.2884\n",
      "Epoch 572/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0063 - val_loss: 0.3337\n",
      "Epoch 573/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0100 - val_loss: 0.3181\n",
      "Epoch 574/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0070 - val_loss: 0.3080\n",
      "Epoch 575/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0060 - val_loss: 0.3108\n",
      "Epoch 576/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0058 - val_loss: 0.2995\n",
      "Epoch 577/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0105 - val_loss: 0.3709\n",
      "Epoch 578/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0100 - val_loss: 0.2877\n",
      "Epoch 579/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0100 - val_loss: 0.3530\n",
      "Epoch 580/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0085 - val_loss: 0.3049\n",
      "Epoch 581/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0072 - val_loss: 0.3379\n",
      "Epoch 582/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0070 - val_loss: 0.3094\n",
      "Epoch 583/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0114 - val_loss: 0.2885\n",
      "Epoch 584/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0083 - val_loss: 0.3401\n",
      "Epoch 585/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0053 - val_loss: 0.2946\n",
      "Epoch 586/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0063 - val_loss: 0.3500\n",
      "Epoch 587/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0104 - val_loss: 0.3157\n",
      "Epoch 588/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0069 - val_loss: 0.3120\n",
      "Epoch 589/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0074 - val_loss: 0.3274\n",
      "Epoch 590/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0064 - val_loss: 0.3063\n",
      "Epoch 591/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0052 - val_loss: 0.3149\n",
      "Epoch 592/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0060 - val_loss: 0.3274\n",
      "Epoch 593/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0054 - val_loss: 0.3138\n",
      "Epoch 594/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0051 - val_loss: 0.3191\n",
      "Epoch 595/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0051 - val_loss: 0.3168\n",
      "Epoch 596/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0054 - val_loss: 0.3261\n",
      "Epoch 597/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0068 - val_loss: 0.3033\n",
      "Epoch 598/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0054 - val_loss: 0.3292\n",
      "Epoch 599/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0055 - val_loss: 0.3111\n",
      "Epoch 600/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0056 - val_loss: 0.3233\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20b59c1a048>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create and train our model.\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30,activation = 'relu'))\n",
    "model.add(Dense(15,activation = 'relu'))\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam')\n",
    "\n",
    "model.fit(x=X_train,y=y_train,epochs=600,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20b58a92548>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd5yU1b348c93yvZdlrLUpXcEKS4gGjF27MYGtqjXEjWWeBOv+jMx1hujXo1euUajxhiMYiwJUQJ2QUVkQYqAIp2lbmHZOjvt/P44M7uzldnG7Czf9+s1r5nnec48c55l+c7Z85xzvmKMQSmlVPxzxLoCSiml2oYGdKWU6iQ0oCulVCehAV0ppToJDehKKdVJuGL1wT169DCDBg2K1ccrpVRcWr58eYExJquhYzEL6IMGDSI3NzdWH6+UUnFJRLY1dky7XJRSqpPQgK6UUp2EBnSllOokYtaHrpQ6PPl8PvLy8vB4PLGuSoeWlJREdnY2brc76vdoQFdKHVJ5eXmkp6czaNAgRCTW1emQjDEUFhaSl5fH4MGDo36fdrkopQ4pj8dD9+7dNZg3QUTo3r17s/+K0YCulDrkNJgfXEt+RnEX0JdtLeKxhd8RCOqyv0opFSnuAvrK7cXM/mQTFV5/rKuilIpTaWlpsa5Cu4i7gJ6S6ASg0huIcU2UUqpjib+AnmADeoUGdKVUKxljuOOOOxg7dizjxo1j7ty5AOzevZvp06czYcIExo4dy+LFiwkEAlx11VXVZZ988skY176+uBu2mOy2VS7XLhel4t79/1rLul0lbXrOMX0z+O3ZR0RV9u2332blypWsWrWKgoICJk+ezPTp0/nb3/7Gaaedxj333EMgEKCiooKVK1eyc+dOvv32WwCKi4vbtN5tIe5a6Kna5aKUaiOff/45l1xyCU6nk169enH88cezbNkyJk+ezJ///Gfuu+8+1qxZQ3p6OkOGDGHz5s3ccsstLFiwgIyMjFhXv564a6Frl4tSnUe0Len2YkzDo+WmT5/OokWLeO+997jiiiu44447+OlPf8qqVatYuHAhs2fP5o033uCll146xDVuWty10Hvs/ZKHXS9SUVUV66oopeLc9OnTmTt3LoFAgPz8fBYtWsSUKVPYtm0bPXv25LrrruOaa65hxYoVFBQUEAwGueCCC3jwwQdZsWJFrKtfT9y10NNLNnCZ6yPmVZTGuipKqTj3k5/8hCVLljB+/HhEhEcffZTevXvzl7/8hcceewy3201aWhqvvPIKO3fu5OqrryYYDALwu9/9Lsa1ry/uArorOR0Af2Xb3khRSh0+ysrKADsb87HHHuOxxx6rdfzKK6/kyiuvrPe+jtgqjxR3XS4JKV0ADehKKVVX3AV0d7K9sxzwlMW4Jkop1bHEXUB3JtkuF+PRPnSllIoUdwGdhFQATJUGdKWUihRVQBeRGSLyvYhsFJG7GilzsYisE5G1IvK3tq1mhETbQsdb3m4foZRS8eigo1xExAnMBk4B8oBlIjLPGLMuosxw4G7gWGPMfhHp2V4VJsGukubwaR+6UkpFiqaFPgXYaIzZbIzxAq8D59Ypcx0w2xizH8AYs69tqxkh0QZ08WkLXSmlIkUT0PsBOyK280L7Io0ARojIFyLylYjMaOhEInK9iOSKSG5+fn7LauxOIYgDlwZ0pdQh0NTa6Vu3bmXs2LGHsDZNiyagN5QHqe4CCC5gOPBj4BLgBRHJrPcmY543xuQYY3KysrKaW9dQbQSPJOHya0BXSqlI0cwUzQP6R2xnA7saKPOVMcYHbBGR77EBflmb1LIOrzOFhEBFe5xaKXUo/fsu2LOmbc/Zexyc/kijh++8804GDhzITTfdBMB9992HiLBo0SL279+Pz+fjoYce4txz6/YsN83j8XDjjTeSm5uLy+XiiSee4IQTTmDt2rVcffXVeL1egsEgb731Fn379uXiiy8mLy+PQCDAb37zG2bOnNmqy4boAvoyYLiIDAZ2ArOAS+uU+Qe2Zf6yiPTAdsFsbnXtGuF1puD2aUBXSjXfrFmz+MUvflEd0N944w0WLFjA7bffTkZGBgUFBRx99NGcc845zUrUPHv2bADWrFnDd999x6mnnsqGDRv44x//yG233cZll12G1+slEAgwf/58+vbty3vvvQfAgQMH2uTaDhrQjTF+EbkZWAg4gZeMMWtF5AEg1xgzL3TsVBFZBwSAO4wxhW1Swwb4nCkkVVW21+mVUodKEy3p9jJx4kT27dvHrl27yM/Pp2vXrvTp04fbb7+dRYsW4XA42LlzJ3v37qV3795Rn/fzzz/nlltuAWDUqFEMHDiQDRs2MG3aNB5++GHy8vI4//zzGT58OOPGjeNXv/oVd955J2eddRbHHXdcm1xbVOPQjTHzjTEjjDFDjTEPh/bdGwrmGOs/jTFjjDHjjDGvt0ntGuF3pZIUrGh0LWOllGrKhRdeyJtvvsncuXOZNWsWr776Kvn5+SxfvpyVK1fSq1cvPB5Ps87ZWDy69NJLmTdvHsnJyZx22ml8/PHHjBgxguXLlzNu3DjuvvtuHnjggba4rPhbbREg6E4jlQK8gSCJLmesq6OUijOzZs3iuuuuo6CggM8++4w33niDnj174na7+eSTT9i2bVuzzzl9+nReffVVTjzxRDZs2MD27dsZOXIkmzdvZsiQIdx6661s3ryZ1atXM2rUKLp168bll19OWloaL7/8cptcV5wG9BRSqaTSG9CArpRqtiOOOILS0lL69etHnz59uOyyyzj77LPJyclhwoQJjBo1qtnnvOmmm7jhhhsYN24cLpeLl19+mcTERObOncucOXNwu9307t2be++9l2XLlnHHHXfgcDhwu908++yzbXJdEqtui5ycHJObm9ui92586Voyty2k6vYN9MtMbuOaKaXa0/r16xk9enSsqxEXGvpZichyY0xOQ+Xjb3EugIR00qik0uuPdU2UUqrDiMsuF0dSGknio8JTBaTHujpKqU5uzZo1XHHFFbX2JSYmsnTp0hjVqGFxGdAltJ6Lp7wU6BHbyiilms0Y06wx3rE2btw4Vq5ceUg/syXd4XHZ5eJKsgHdq0kulIo7SUlJFBYW6rDjJhhjKCwsJCkpqVnvi8sWujsxBQBvpa7nolS8yc7OJi8vjxYv0HeYSEpKIjs7u1nvic+AnmSzFvk0oCsVd9xuN4MHD451NTqluOxySUixN0L9mihaKaWqxWdAT7Z96H6vLtCllFJh8RnQQ10uAY8GdKWUCovLgC4JNqAHtYWulFLV4jKg4w5N99c0dEopVS1OA7odtmi0ha6UUtXiNKDbFrr4NMmFUkqFxWdAd4UCul8DulJKhcVnQHc4qJJEHBrQlVKqWnwGdMAniTgDGtCVUiosfgO6IwlXoHk5/5RSqjOL34DuTMYV1ICulFJhcRvQA84k3BrQlVKqWlQBXURmiMj3IrJRRO5q4PhVIpIvIitDj2vbvqq1BV3JJAY9BIO6prJSSkEUy+eKiBOYDZwC5AHLRGSeMWZdnaJzjTE3t0MdGxR0JpEsZVT6AqQmxuUqwEop1aaiaaFPATYaYzYbY7zA68C57VutgzPuFJKoosIbiHVVlFKqQ4gmoPcDdkRs54X21XWBiKwWkTdFpH9DJxKR60UkV0RyW52txJ1CMl4qNaArpRQQXUBvKJNr3Y7rfwGDjDFHAh8Cf2noRMaY540xOcaYnKysrObVtC53MslSRbnX37rzKKVUJxFNQM8DIlvc2cCuyALGmEJjTFVo80/AUW1TvcY5ElJI1i4XpZSqFk1AXwYMF5HBIpIAzALmRRYQkT4Rm+cA69uuig1zJKVql4tSSkU46PAQY4xfRG4GFgJO4CVjzFoReQDINcbMA24VkXMAP1AEXNWOdQbAmZCKWwJUeHT6v1JKQRQBHcAYMx+YX2ffvRGv7wbubtuqNc2VaNdE91ZqkgullII4ninqDiWK9lWWxbgmSinVMcRvQA8livZ5NKArpRTEcUBPSAq10Ks0DZ1SSkEcB3Rnom2hBzzah66UUhDHAT2cVzSgiaKVUgqI54CeYEe5GK+20JVSCuI5oLvDAV1b6EopBXEd0G2Xi/FpQFdKKYjrgG5b6OLTmaJKKQWdIKA7/BrQlVIK4jmgu5IADehKKRUWvwHd4cAriTgDmihaKaUgngM64Hck4QpoC10ppSDOA7rPmUJCUAO6UkpBnAd0vyuZJOPBFwjGuipKKRVzcR3QA64UUjQNnVJKAXEe0IPuVFLEQ4UmilZKqfgO6MadQioebaErpRRxHtBJSCMFjyaKVkop4jygS0IKKVJFeZV2uSilVHwH9MQ0e1PUpy10pZSKKqCLyAwR+V5ENorIXU2Uu1BEjIjktF0VG+dMSiMVD5XaQldKqYMHdBFxArOB04ExwCUiMqaBcunArcDStq5kY1xJ6TjE4KnURNFKKRVNC30KsNEYs9kY4wVeB85toNyDwKPAIVtcxZVk84r6PBrQlVIqmoDeD9gRsZ0X2ldNRCYC/Y0x7zZ1IhG5XkRyRSQ3Pz+/2ZWtKyE5AwC/ttCVUiqqgC4N7DPVB0UcwJPALw92ImPM88aYHGNMTlZWVvS1bIQ7OQ2AQGVpq8+llFLxLpqAngf0j9jOBnZFbKcDY4FPRWQrcDQw71DcGJXEUECv0ha6UkpFE9CXAcNFZLCIJACzgHnhg8aYA8aYHsaYQcaYQcBXwDnGmNx2qXEkt+1DD1SVt/tHKaVUR3fQgG6M8QM3AwuB9cAbxpi1IvKAiJzT3hVsUoIN6Hi1ha6UUq5oChlj5gPz6+y7t5GyP259taJUHdC1ha6UUnE9U1QDulJK1egUAV18FTGuiFJKxV58B3R3CgBOvwZ0pVSM5C2H3atjXQsgyj70DsvhxCtJOP3a5aKUipEXTrTP9x2IbT2I9xY64HMm4w5oomillOocAT1YSTBoDl5YKaU6sbgP6AFXCqm6JrpSSsV/QA+6U0nBo1mLlFKtt3MFLLgbTHz+xR/3Ad24bRq6Mg3oSqmW8nttEH/pNPjq/8DfglXA/3xmzeuAr+3q1gxxH9BJSCVVW+hKqZYK+OChLPjgN1QvLhtsQTzZ9rl93r0aHuwBG95vsypGK+4DuiSkkYJHW+hKqZbxhUbJ5f4ZJBTQW9PCzltmn79rMj1Eu4j7gO5ITCNFPJRX6U1RpVQLVLfGBSQUEv1VLT+f0x0676GPSfE9sQhwJqWSRJV2uSilWqZWazzcQvdG9966N0/XvAnzbrGvg4e+Hz3uA7o7KZ1UqaLME+U/gFJKRQoHbxFqAnoUwdhXCeKsve+tayLOe+gDetx3ubhS0gGoqtA10ZVSLRDZGg93uRyshb5uHjzcG3Z903iZltxYbaW4D+juJBvQfZUlMa6JUiouhVvSVSX2ARA4SB/65k9Cz582XqaqBDwlUJZvu2YCPvjnz6FwU6ur3Ji473JxhAK6XxNFK6VaoqHgfbDukuSu9rlsb+NlNn8Kj4TSMZ/+KHzxNJTkwd51cP0nLarqwcR9C52kLgCYyuIYV0QpFXd8Hnhuev394S6Xt66D1X+vfzwp0z6X74vuc/79XzaYA+xaAcv/0vy6RiH+A3piBgBBT+yXrlRKxZmqRv6yDwf0NW/A29fWPx7uHy+LMqDXldKtZe87iPgP6NUtdA3oSnUou1d13DVR8r+3QwwfH9bw8Q3v2+UA6vrhQ9j8Wc2koQM7W/b5XQe37H0H0QkCum2hV9/MUErF3g8f2K6MFa/UP1ZRBPd1gZWvHfp6hc2eUnuIYV1Ln4UDO2q2wzcyX70AXjkHdi632+FulObqOqhl7zuI+A/ooS4XpwZ0pTqOcADcu7b+saLN9vnr59v+c/9+Fbx+Wducq3Bjzev/ndTyvzacCXDtR7X3Jaa1vF5NiCqgi8gMEfleRDaKyF0NHL9BRNaIyEoR+VxExrR9VRuRkEYQBy6/jnJRqsMIr4ligvWPhfdJO7Qn177TdmuofPPX2tvzbm7e+y970z4POxmyc2r2//L71tWrCQf9iYqIE5gNnA6MAS5pIGD/zRgzzhgzAXgUeKLNa9oYhwOvK5WkQDlVfl3PRamOIRTQaaBVG27ptjage8vhqz9CsIEvjaasmhtdufX/qr39zZz6ZdJ6N/7+QcfBUVfBmXXCYXoT72mlaH6iU4CNxpjNxhgv8DpwbmQBY0xkf0cqDf4rth+/O50MKedAZWzWIFZK1VHdQm8ooLdRC/3jh2DBnc1vkb9zfes+N9LAaTWvJ19X+5g7Cc5+CjL62O3/eB/Oe7btPrsB0fxE+wERdwfIC+2rRUR+LiKbsC30Wxs6kYhcLyK5IpKbn5/fkvo2KJCQQQaVHKjQgK5Ux9JAQA8vWhUO+i1VXmCfvU0s+5G/AfKWQ/EOmHOBHaXSlk55oOZ1OHA3ZsBUmHBp235+HdEE9IZ+6vX+lYwxs40xQ4E7gV83dCJjzPPGmBxjTE5WVlbzatqUxAzSpYJibaEr1TE01UIPDwcUB+xYBt6K5p27shi+/F+qw9COpXbUzHu/qikTDMKrF8PsyfDCifDpI7DxQ3j39mZfSpMyB8AxofZre9wTaKZoapAH9I/YzgZ2NVH+deC81lSquSS5CxlUUKwtdKU6iCb60MNT7csL4MWT7fomzfHRA/D+r2syAoVnci77U00ZXzn8sLBmOzwEsWxP8z4r7NK/wykPwrBT6h9rqAupHW98NiWagL4MGC4ig0UkAZgFzIssICLDIzbPBH5ouyoenDO5C+lUUFyhS+gqFXPBoB1tAo200EP5OgtCQW/XioOf881r7HT5bV9C7ot2X1VoMqGvvH75qjrdMFs+s8/RrnOemAG3roQzHrfb/SfDsXV6krsNtc+ZA+xzRj/7Pmdiu974bMpBF+cyxvhF5GZgIeAEXjLGrBWRB4BcY8w84GYRORnwAfuBK9uz0nW5UzL1pqhSHcWq12Dr4tBGQwG97mJYUfSlf/umfQw6Lro6vHpR/X3OhOgD+o1f2EA95Tr7iDwHwEUvw4jT7evJ19myI2bAqDPrnepQimq1RWPMfGB+nX33Rry+rY3r1Szu1EycVFJc3oq0UUqp5tnxtV2kqnI/JKRC77F2f0lEj2xTLfQwEdi+FNbPg2k/h4y+dv++9Xao4OCIxbOqvygOYu+a+vu6ZNdMahp7AXz7VsPvFWdNq7uuMx+3Nz9HnVWTas7hgJGh4O5Ojq5+7STul88FkKQuOMVQWa7ruSh1yLxYpz/5vtD/v1qTiZq4KVpN4KVT7cslz8BZf7BrNL15dc2+thBuXfcYARe+ZFv78++onyquoclQYRl94cz/aZv6tINOEdDDC3R5SotiXBGlFCZigl9DM1LqttCL6iR8ePcXrfv87CmQ93XNdv+jYcdX0Gc89BwNP77b7s+5GkaeAfu3wF/OiVgXvYMuKBaFzhHQU+0QyGBpC5eyVEq1nchs9+HWbuleO168+9AG+tDb2LSboM9z8PREmPEI5FwDix6DqTdAavfaZdN72cf1n9qEFAvvht7j2rd+7ahzBPT0XgA4ol1sXinVOk0tVBXZQg93Zzwz2Y5KGXkmfP9e29en/9F2BcSgz36hdBtS0wUEcOI9Tb+/1xj76DsRska2ff0OkdiPhG8LofUUEjxtN/tUqU7BX2Wz07dGMFg/gO/f2kT5iIAe8Nrt8BDD1gTzjOz6+3KugbEXwqWv1wTtYCvWdBo4rd2STxwKnSOgh7pc0n1FeHy6QJdS1Z6aYLPTt9Rnj8IDXeGdG2ovgvX0hIbLL30Ovny6Znv7Utj7bcs/H8Dhhplz4NoP6h/rdQRc+KLN8XnMrfCT52Dcha37vDjWOQK6KwFPQld6yn4Ky3VykVLVSpua1H0QwQB88rB9vfp1G9iXvww7G5kI5Ku0C2ZFKttj1ygHGH22Dc7N9eu99r0ZfeHny2ofi+wecThh/Cz7fJjqHH3ogD+pO90qSykoraJfZmzHgioVl0r3QEp3O776q2cha1T9Mv9qYspJY38JFG2247pnzoEti+AvZ0dfpzHn1g7QWSNqXl+9oPZqh6rzBHSSu9F1fxkFZTq5SHUCq+ba1fnaKVVZPb5K+J+RMOlKOOtJWFAvj03rDDjGPqf2bLzMqLNqlsL95fd2ko47pX65y9+2U+t7HdG2dewEOk1Ad6R2I1N2s10Duop3waBdszu1J9xxiJZFCi9Fu+ZNGHJ8258/PBQwa6Tt537nZ7WPX/gSDDwWyvbaJWabWgtl2EltX79OonP0oQMJ6T3oKmUUlGkfuopz/tColKaG4a5/t/G+7IbsiJho09AokIpQQPeVw5v/Ef15G3LSvTWvf1MAP3keJl9rt0VsP3fY9Z/ZmZdjL7BB/NoPIaeVn38Y6zQB3ZXWna6UkV/iOXhhpTqyaIYZzr0M/nSCXQe8qSGEYeFp+mX58MgAu/xsZGAvL2xRVRuUEJEA2emG8TNt9p6G9J1QE+xVq3WagE5yNxLFR3FJcaxrolTrRAZ0b7nNuNOU3atqXpfugbJQy37f+trljLHJILxlNkHEg1l2gapPH6lZtCrSUVc3/Hm3rIA7t9asDS6hm5bJofHbrkaCd6TrPoFz/+/g5VSzdJo+9PBkgJJCnS2q4ljBxpouF4C3rrOTce7cBsmZdl/dxa3Ck34qiuyNTYAJl8PKOkmN7w+9X5ww8Bi7cmFT3SuTfgobFkDpbrs99CSY/is7fR/siBiASVfY4YxXvWfPOeEyO/bc28A65WH9JtmHalOdJ6Cn23x+3uKdMa6IUi3kq4RnjqpJnAA1MyurSmoC+kN10zeGAvqjg2t21Q3mkbJGwWVvwsN2yQxGnA4b/l2/XGjRu2qn3F97nZOp18O6f9jFrs5+yu7rNcY+n/FY45+v2k3n6XLpYqcFZ3r3auYiFZ8qQquF1l19EMBTYp9Ldtc/FgxAwB/95yR1qd2nffx/2ecu/eHuvJr9iRl2H8Bxv4JeY2ufp99RdtJPjLLzqPo6Tws9ox8AfaWAbYUVZKYkxLhCqtOYcyEcOROObCALTlsI+GHu5TC8gXyVYZ5i2LAQ9q6tf8xbBps+iv7z+k+2z7P+ZlO69Z1ox54PPxUS06HLADiw3b6e9Sps+rj2yBTVYYlpatW0dpSTk2Nyc3Pb9JyB/x7AXyum0v2ipzh7fN82Pbc6TBlT0/d8XzslUNm/FZ4a3/bnHXMurPunfS1Ouwpiahb85/qabDsNKd4OW7+ACZe0fZ1Uq4nIcmNMTkPHOk+XCyCZ2fSTArYXVcS6KqqzCNTJZrPsRTtUcO0/Wn/uqlJ7rjkXtP5cUHOTMuziV+wYcLC5Lo+6Gn62uOlgDnaavgbzuNSpArojsz8DnUVsK2zi7rpSTTEGPn64pmvDX2dM+PI/2+e/Xwl71zV+ng/vs8MBGxLw28/IC/2FWrixVVWuNua8mtfhBMauRPvcdSCc/QebD1N1Wp2nDx2gSzZ95HO2FWoLXbWQtxwWPWof0++oP+klsoeyosB+AexbXzO6Y9sSm7vy8yft9o/vsuPCnz8BznkKdn0Dix63adjqjiJpjWs/hjVv2NenPgzH3Gxfjz4HTn/UDkFUnV5ULXQRmSEi34vIRhGpt2qPiPyniKwTkdUi8pGIDGz7qkahSzbppoyCwjac9aYOH8bAv++s2V70mM1oX7tQzcuAz679/ew0G6gB/jwDXjix9ls2fwYlebZr5eOHanJqelrYJ3/cL+0CVZGyj4Ip10PmQBh7fs1+hwOm/izm2ejVoXHQFrqIOIHZwClAHrBMROYZYyL/3vwGyDHGVIjIjcCjwMz2qHCTQkOs3GU78PgCJLkP33WRVQuUF9Qfv71/W83rRY/VTtaw+VPYs9q+3rIY+jSS9KGqTuB2uOtnmg+7YzM8NsS+nvkqdB9mhxguuBu+nw+3r7VDdCMHM/zodvvcfSj8YnWTl6g6t2i6XKYAG40xmwFE5HXgXKA6oBtjPoko/xVweVtWMmqh9ZuHsZO8/ZUM65l2kDeow1LJLqgsrukmCVv/z/plX4tol9RN3vDl05CQbl9/8Bv7qOuDe+GLp2rvu22VnST036GRWGc+YfvsJ11hkxj/5Hn46v/sjUwRW+aCF2FnbvV8C0Tg1Ieg62AYfVZ01606vWgCej9gR8R2HjC1ifLXAA1MOwMRuR64HmDAgAFRVrEZegzHiJMRjh1sLyrXgK6sqjJ493Y47WFI6wlPjLb7I4chlhfAe79s/rm9pQ3vd7gg6K8fzMFm3gkHaoCjrqqdxGH8TPuIlJACg6fX3nfMLc2vr+rUoulDlwb2NTh4XUQuB3KABuf9GmOeN8bkGGNysrLqTl9uA65EAl2HMFLy9MaoqrF6rr1h+NnvGy9T3oYJxi96uWYqPECPEXDRX2DqDTDouNrBHA7rlGmqbUXTQs8D+kdsZwP1EhWKyMnAPcDxxpiYZZlw9hrNyMJlfKVj0Q8fAZ/tz25spmX1WPI6gdTnqZkCv2FByz//ghfhrWvs65Pvt5l3tnxmt4efalOvuRLhiPNqv++SuTU3U5VqA9EE9GXAcBEZDOwEZgGXRhYQkYnAc8AMY0xMlzuUnmMYsP5f7NhbFMtqqPZUXgCr34Cjb7St3U8fgcWPw/F3wcgZdip7/vdQtMV2efQ50r6vbkv44V52tEhGXztuvLmyJ9tkDOMutKt9JmZAdmgC37CT4cYvm06TNnKGfSjVRg4a0I0xfhG5GVgIOIGXjDFrReQBINcYMw/bxZIG/F3sn5PbjTHntGO9G9dzFA4MVXvWA8fFpAqqnb19vV27ZOHddl3u8Lrfnz1iHzPn2LVRwrZ/aZ+Ld8DTE2uf6/MnwQRbVo9rP6x5PfTE+sc156U6xKKaWGSMmQ/Mr7Pv3ojXJ7dxvVqupx250KNiMwVlVfRIS4xxhVSLbP0C1r4DZz5utws22hua5fm1F6Las8befIw0t5FBVuGlaGt9zuKGy17+FuxaCR8/WHv/yffDh7+tnZVHqQ6ic80UBeg2hKDDzQhHHut3l3Dc8Ha4+ara38tn2Odjb7PZdF45xyYRrrteyaaPWz91/vwX7MSc/O/howdg3zrbZTLsZLvK4Zf/C9NuhqPEBwIAABvuSURBVCXP2BEpw06G1B6t+0yl2kGnWm0xzD/7GL7aA2tP/is/O37owd+gWibgA3G0bpTG13+CLYtg5l/tdmWxHSf+7DS7HV4lsKUmXgHf/LVm+5QHoHATjJhhZ2zuWwcn/rrmeEWRXf0wnE0nGLTlXEk2uIcyYykVK02tttj5WuiAa9gJTMn/I+/k7QE0oLebB3vY1urlb9XeX7QFXrsErnin/mJQwYD9AggG7MzLT39n9794GuzfAmV7a5dvSTAfdzFs+wLSesHJ99nsOQ+HkjAce1udwufX3kzpVjtoOxx2DHj4mFIdWKcM6Aw7iYQlz1CxYTEe39G6BEBLeStsd4YJ2JEjDdn4Ye1tf5XNRl+5H54YZRMKJ3eFgNfu+7+jbWKFvGU1C1gB7Piq5fU86bc2a87HD9s1U/pPgQv+VLvMjUuaznGpVCfQOQN6/6MJOtyM965hdd4BpgzWllWLvHl1zfjse/bWTlsWuU54yS479A/gk4drL2iV+xKs/xcUR6yJ8nqtUa9NyxoNV8+vnS8zox/8bJHNwVm4EYaeYPdnT4HXL7GJHeqqO81fqU6oU62HXi0hBW+viUxzrGOrro3eMt/MqT3ZpqLAPvu9kL/B9nWHPXmEHZUCNc9hS56pHcwbExqdxHG/ggtfssH59MfgmoW2q2NiaOTKxa/ANe/bm5KZ/WuCOUCPYXDzMjsaRqnDUOdsoQPuYcczdtfjLN6zm9oTXVWTvBW2L/ufP6+9v2yfXRjqk4fsZJ2zIrpLTLBmVEpjTnnALlRV15lP2BuQvY+053G47GShsXWy+JzxOEy9EXqPrX8OpRTQWVvogHPUGYjAhB+ejnVVOr5Fj8P/HmWD+ctnwLPH1C/z9yth9d/tMrFgF7uK1qQr7WiT/qE13S6Za2+knvkETL7G9s87nDY1Wt11TsLcyRrMlTqITttCp98klqWdyNgDn9mhZ45O+911cAE/OOv8Ux/Is10l0++wo00A/ruJ9GTF2+Htaxs/HnbLCvhjaIbusBPBnQLnhL5Urw4twqmLUSnVLjp1lPMMPJFMSti3cVmsq9K2Aj7bjx1WvAMW3mP7t+vauw4e7A4b3ofFT8C7/2n3b1tinxc1uDBm0yZcBj1G1t9/+Vs2ycKdW+yU/Jlz4Pzna447nBrMlWpHnbeFDvSeOAPW3sO+b/5NzxFNLeEeZz68z95sDGeveeUcO5ty1Fmw9FnoOgiO/jnM/6VtIQP87aKa9x91ZfMXozrzCZs1fusi6HmEnWzz9fN2RMmq1213yLDQChAuXW5BqVjolDNFw4JBww8PjCcj0UGf/1pm+2jj2Y6v4atnoWCDTYV2xT/sDcVHQslCUrpDRZT5VMNp0FK6226SoB9Kd8N378HAY6B0D7x9nS3bf6odWaKUirnDbqZomMMhfNLram7Ye59t0f6oGTfyOqJXL7LTz8P++XMo2VmzHW0w732kTcLQbYidsRnuX0/tAb3H1ZQbeIztaw+l9lNKdWydug8dIPHI8/gkMJ7AF8/UngzTUe1ZY2dbgh1R8t4vbULgjx6sHcyhJphPu9k+6hpyAkz6qU1Vdt3Hdt9P/wk3LLZ93SL1b5ZG6pINA462+S+VUh1ep26hA/x4ZE8efO8UTqh83C4ENe2mWFepcYWb4I8/ssG52xB4L3QD0xjIfbF++ZlzYPhp4Eqw26c+BN4yOztz8rXgTKwdsOvO9lRKdSqdvoU+uEcqFQNP4gtHDuaD39jsNoWbDm0lvnym4WTBkYJBWPOmfb3kmZpgDjaYZ42y2XVcyXbf5W/B6LNrgjnYFndiul2AKiG1futbg7lSnVqnb6EDXDJ1IDe/fi2f932a1E9/Z1f4i8z43t7ev8c++yrh6JsgKQNWzYUFd9kbjjuWQmWdlHkjTocNoXHbfSfZLhMR+PWeQ1dvpVRcOSwC+rQh3dlPBgtG/54Lvjjb7vz9YLjsTZvYoC0ZYxMleMvsdPm939YcC3+ZhJMlgA3aGf2gMlTm1IfsSoQXvGRb2AUb7Zoljc2gVEqpkMMioGelJ9I1xU1uaSYX3PoNPDPFtohfPsMGVxOE9fPgvGftqI8+42vWwI4UDNqx1wOm1l5OtrwAlr0IXQeCpwT+fUfTFQoH87BbVkDpLjtBaMjxtY/1GNayi1ZKHXYOi4AuIhzRtwvLtu7HdB2H/GKNzSX50YM2W3zYi6fY5+Gn2aS/hRth2Z9g0HH1c092GwpFmyC5W/3ukrDELlBVp2snIR28pfb1kbPgxHts33a3IfahlFIt1KknFkWa89U2fv2Pb5lzzVR+NDwiH+TK1+AfN0R3kshg3JSJV8DUG+zsSb8XvnkFfB47hNCVaCcIeQ7A6LNadjFKqcNWUxOLDpuAXlbl56ynF5OS4GL+bcfVHDAGNn8KA6ZBoMp2qexcYUeV5C2DrJHQ6wjY861dwnX7lzZpcMlOO2b8l9/bhav6TrT5Lws22Pdon7dSqh20eqaoiMwAngKcwAvGmEfqHJ8O/AE4EphljHmzdVVue2mJLs6flM2TH26guMJLZkpouJ9ITZIEd5JdfbApg35kH2C/DERs+rOwnjqrUikVGwcdhy4iTmA2cDowBrhEROrm89oOXAX8ra0r2JaOHtIdY2DRDwVtc0JthSulOpBoJhZNATYaYzYbY7zA60CtpI3GmK3GmNVAsB3q2GaOGtiVfpnJzFmyjVh1NSmlVHuJJqD3A3ZEbOeF9jWbiFwvIrkikpufn9+SU7SK0yFcd9xgvt5axNxlOw7+BqWUiiPRBPSG+hVa1Lw1xjxvjMkxxuRkZWW15BStdtnRA5k8qCu/X/Ad/kCH/oNCKaWaJZqAnkftLMvZwK72qU77czsdXH3sYPZX+Phsw6H/K0EppdpLNAF9GTBcRAaLSAIwC5jXvtVqXyeO6smQHqnc8863HKiIgyV1lVIqCgcN6MYYP3AzsBBYD7xhjFkrIg+IyDkAIjJZRPKAi4DnRGRte1a6tZLcTp6aNZF9pR6e+uiHWFdHKaXaRFTj0I0x84H5dfbdG/F6GbYrJm6My+7CxTn9eemLLSzbWsTzPz2KPl2SY10tpZRqsU6/HnpT7j/3CMb3z2TNzgPcMGcFwaAOZVRKxa/DOqAnupzMuWYKd5w2klU7irnnH9/q+HSlVNw6LFZbbEp6kpsbjx/KvhIPf1myjSWbCnj56ikM6pEa66oppVSzHNYt9DCHQ7jvnCM4c1wfthZWcOofFrFqR/HB36iUUh2IBvQQEeF/L5nIHaeNxOsPcu7sL7j4uSW8unQbhWVVsa6eUkod1GGzfG5zFJRV8dC761iwdg8eX5DuqQnccPxQ3lyex4Z9pbx01WROGNkz1tVUSh2GdD30FvIHgqzfXcqdb61m3e6SWscumTKA8yf1wxcIMm1Id0RXXlRKHQIa0FvJHwiyraiCSm+AW1/7hi2F5UT+2LqnJnD8iCx2FleS6HZy5bSB/LCvjF4Zifxkoh2e7/EFSHA6+Oi7faQmOin1+DntiN6NfGLTPv1+H0N6pDGgewN5T9vA5vwyVucd4LyJLVqDTSnVjlqd4OJw53I6GJqVBsDHv/ox/kCQl7/cSnqSi0pvgFe+2sbb3+ysLr8oYo0Yrz/Iyh0HeO3r7fXO+92DM0hyO5tVF18gyFV/XgbA1kfOjPp9G/eVUuUPckTfLgcte+bTn1PpC3D2+L44HfqXh1LxQgN6C7icDq49riah81XHDmbZ1iIyk90cqPRx62vfMKZvF/aWeLjzrTWNnufJDzfQOyOJQd1TmfPVNj76bh+nHdGLRy8cT5dkd4Pv2bivrPr1jqIK+ndrupVeVO4F4OQnFgHRfQlU+gLV781KTzxoeaVUx6ABvY1MHtSt+vWXd58EgDGGhWv3sL/Cx/mT+pFfWsX2ogoeenc93+0p4bnPNtc7z8K1e/lq8yecdWQfjujbheJKL1W+IIN6pDC8ZzpfbKzJtvTkhxv4n4vGU1zh4901u7k4J5tEV+0W/3G//5iKUIAG+xeD2yl8uiGfY4Z2r1c+UkFZFQu+3c2D767n2/tPI8Glg6KU6sg0oLcjEWHG2D7V29ldU8jumsL8245jZ3Elq3cU079bCut2lfDxd/volpbAxP6ZPPXRD7y6tH4XTdikAZlMHtyN5z7bzPtr91Lu9WMM/OYf3zJlcDdG9U4nPclFSoKLcm+g1ntztxaxr7SKX8xdyZXTBnL/uWMxxvDBur1MG9qdFz/fUl02v7SK+/+1Dn/QsH53CcN7pbHngIchoe4njy/Ac59t5stNBfxh1gRdC0epGNOboh2U1x9kzwEPmwvKGNg9laLyKuat3EVBuZffnj2GbikJ3PTqCpZsKqRXlySGZqXiEGFncSXf77H95dG46/RRVPmCPPnhhnrHcgZ2JXfbfgAunTqA99fupaCsiouOyqbE42Ncvy48/r59X3qii2cum8TxI7J4a3keT3ywgV+eOoIxfTMY1TuDzflleHxBxvTNaPXPpqjcy8K1e5iZ0x+H9vGrw4yOcjnMVIZa5XtKPKQmOklJcIW6f/ayaEM+efsrGN8/k78t3V4v8I/tl8GffprDtN99HNVnZSS5OGVMb95akQfAscO688XGwlpl7j59FL/793cAvH/7dEb0SqewrIqAMdw4ZwV3nz6KnFCXVanHx94SD8N6ple/v8Lr57zZX3DexH58t7uUeatsfpX/d8Yorp8+tF6diiu8CEKXlIbvQyz+IZ8dRZVcOnVAVNfYEsYYVmzfz6QBXXVIq2pTGtBVgzy+ACUeH/mlVWRnplDi8VXfZP30+338sLeMI/plMK5fFx5+bz39u6XgEGFErzTeW72b7UUVXHXsIHIGduOPn23C7RT+smQbXn+QiQMy+WZ7/eUT3E5haFYa3+0pRYTq4Z8/GtaDzBQ3y7YWkV9axYmjejGmTzoje2fwRu6OBrNLicAfZk5gc345S7cUcv85Y3ls4Xd8uH4fPdISyP31KQ1e96C73gNg83+f0WALf92uEj7fmF/9ZVFc4SUt0YXLGf09hH+t2sUtr33DH2ZO0OGfqk1pQFeHTDBo2FPiIT3JxQuLt/DTaQN5del2eqYnMnVId/746Sa2FpazdEsRvTOS6N0liY37yqjyB+iVkUTfLslsLSynoKyKyNWMUxOc9e4HHMzEAZn4AkHOGd+XY4b2wBsIkux2cvpTiwH4rxkjmZnTnyp/kP0VXr7ZXsyfFm9mW2EFAJ/86sf075rMxAc/ICPJzWd3/BiPP8jGfWWM6p3e5JDT/56/nucXbea2k4Zz+ykjmv+DVKoRGtBVh+YPBPH4g6Qlumrt21ZUQXmVv7rvvdTjY/6aPSS5HYzqncHbK/L4eksR47K70CXZzcZ9ZRgDn4dGAnVJdlPpC+CN8n5CXSkJTo4a2JXFPxTUOzZrcn/GZXehe2oiM8b2psofqDVi6PpXcnl/3V7+49jB/Oas0XyxsZCcQV2rvwS2FZazvaiC44bbZOm+QBB3M/4CAHhlyVZEhDPH9aFbakKLrlHFHw3o6rBTUFZFSoITQfhyUwE7iipwOh0s31rETScMI3frftbuOsCqvGJSE1ws3VJU/V63U7jjtJEs/qGgwWA+oFsK24sqqrfH9evCmp0H6JmeSI+0xFrLRHRNcTN9RBb/XLmLqYO7ccqYXuTtr2TBt3vYU+LhgXOPYEdRBS9+voVHLjiSi3NsPvZdxZV8s72YM4/sQ6U3QJLbgYhQ6Q2wfNt+Jg7I5IjfLgSgT5ckltx9EsGg4ctNhUwd0g2300EgaFjw7R7Kq/ycOLonl/1pKY9fNJ5x2QefXBaNCq+feSt3cbHenD6kNKArdRDGGESEjfvKGNAtpXrM/eIf8lmyqZAhWWlsL6rgFycNZ8nmQv7zjZXVSze8v3Yvxw3vwYa9pazKOwDAyF7pnDKmF88t2oQvEP3/seyuyQSCht0HPNX73E5hTJ8Mjh+RxeKNBQ3emzh/Uj/Kq/wsXLuXaUO6M2lgJrlb91d/UV37o8G88PkWpo/I4omLx9MtJaHRIPze6t38+YstvHrdVBKc9ovk250HKPH4OGZoDz5av5c5X21j7a4S9pVW8fBPxnLy6F70ykiq9fOc89U2ktxOLgp9SUUqr/LzzfZiJg/u2uRcCFWfBnSlDgFjDCWVfrshtstn94FK1u4sYUD3FLYXVhAwhm6pCSzftp8RvdJYk1dC11Q3ZVV+Xvp8KwO7p+ByCD/sK6ue5QuQluiiwuuvvq+QlugiZ1BXPv2+/s3iaAzukcqR2V0Y1TuD/NIqPt+YT2ZyAntLPdX3EEb0SqPU46dnRlJ1foCfTR/Cc4vqT4gD+PiXx/PN9mISXA4SXA5+9tflAGz53Rm1Rvr4AkEu/OMSmyXsjNHMnNKfjKSGRyQ1xh8IsnrnAcZnZ0a1PMWiDfns2F/BZVMHNni8wutndd4BKn0B3lmxkydnTuiwy15oQFcqDhlj8AcNLocgIpR4fLgdDpLctjslPOpmf7kXl1OYt2oXPxrWg6+3FFFW5Wd7UQXj+nVh6pDuvLMij0pfgK0FFWzKL6NnRhJfbynE4wvidAiB0DdFeqKL0ip/g/XpnppAYcSXTF2JLkeD8x/On9gPESG7azKfbyxgW2E5BWW1z3PJlP54fEFytxWRluhmdO90xvbrQlqii2lDu9vzux18uG4fByp9fLnJdodN6J/JM5dOJLurHZ3l8QVYnXeAowZ2ZU+JBwH6ZiZXj2yad/OxHJmdWetn7AsYHl3wHS9ETKp77bqjqz+3rmDQ4A0EKSirItHlZHtRBVsLyjl6aHcWb8in0hdg2tDuVHgDTBrQtdGfV0u1OqCLyAzgKcAJvGCMeaTO8UTgFeAooBCYaYzZ2tQ5NaArFVseX4Cq0M1oYwwBYxCEPQc8DOiewmcb8hnWM40yj5+CsiqOHtKdD9btoXtaIn26JDH7k42cOa4vSzYXcGR2Jq9/vZ3RfTJIcDlwOx2kJ7n47by1GAMJTgfeQE2wv2zqALqmJPDMJxvr1atneiL7SptOKpOa4GRYr3R+2FuKxxegT5dk0hJdbMovwx809M5IYk+J7bY6b0Jf/rHSzl0QgXPG98UpdvmLonJvg19EEwdkcv6kbEoqfby9Io9jh/WgV0YSy7ftZ1thOZvyywH7JRYMfSm4HIK/TqL5xy48krOO7EuS20F+aRUFZV5G90knaGjxXwCtCugi4gQ2AKcAecAy4BJjzLqIMjcBRxpjbhCRWcBPjDEzmzqvBnSlOr+ici/pSS5cDqGgzI7nzy+tIrtrMgYIBA2r8ooZ3CPV/tXhEJITnGzYW4YvEGRrQTlV/iAHKn2UevzsL/dyZP8unD2+LxlJbnYUVTB32Q52FVdSVOFlaFYaXn+QrYXlFFf4WLPzAKkJTgb1SOWh88by7urdzPlqG26ngwHdUvAHg+w+4LHLWFxxFOOzM/lsQz7/7501eHzRj4665keD+XpLEWt2HmjweI+0RIorvPiDBqdDePi8scya0rKJba0N6NOA+4wxp4W27wYwxvwuoszCUJklIuIC9gBZpomTa0BXSrU3fyBYb0KYLxDEKVJ9UzgQNJR5/LVmFlf5A+zcX1nd4k5JcPLd7lL8QcOm/DL+49jBbC4oo7wqwKb8Mi7O6Y8AHn+ApZuLyNtfwZlH9uXD9Xv5YW8pReU+gsZgjKFvZjKnHtGbCf0zaYnWrofeD9gRsZ0HTG2sjDHGLyIHgO5ArTFfInI9cD3AgAHtN+1aKaWABmf31h3v73TUXyYi0eWsXoQuLNxPHxbOLTBlcM1KqykJLk4YVZOe8uIGRvi0p2hmMjTU0VO35R1NGYwxzxtjcowxOVlZWdHUTymlVJSiCeh5QOTXTDawq7EyoS6XLkARSimlDploAvoyYLiIDBaRBGAWMK9OmXnAlaHXFwIfN9V/rpRSqu0dtA891Cd+M7AQO2zxJWPMWhF5AMg1xswDXgT+KiIbsS3zWe1ZaaWUUvVFlbHIGDMfmF9n370Rrz3ARW1bNaWUUs2hSSKVUqqT0ICulFKdhAZ0pZTqJGK2OJeI5APbWvj2HtSZtBTH9Fo6Jr2WjqezXAe07loGGmManMgTs4DeGiKS29jU13ij19Ix6bV0PJ3lOqD9rkW7XJRSqpPQgK6UUp1EvAb052NdgTak19Ix6bV0PJ3lOqCdriUu+9CVUkrVF68tdKWUUnVoQFdKqU4i7gK6iMwQke9FZKOI3BXr+hyMiLwkIvtE5NuIfd1E5AMR+SH03DW0X0Tk6dC1rRaRSbGreW0i0l9EPhGR9SKyVkRuC+2Px2tJEpGvRWRV6FruD+0fLCJLQ9cyN7S6KCKSGNreGDo+KJb1b4iIOEXkGxF5N7Qdl9ciIltFZI2IrBSR3NC+ePwdyxSRN0Xku9D/mWmH4jriKqCLzW86GzgdGANcIiJjYlurg3oZmFFn313AR8aY4cBHoW2w1zU89LgeePYQ1TEafuCXxpjRwNHAz0M/+3i8lirgRGPMeGACMENEjgZ+DzwZupb9wDWh8tcA+40xw4AnQ+U6mtuA9RHb8XwtJxhjJkSM047H37GngAXGmFHAeOy/TftfhwnluYuHBzANWBixfTdwd6zrFUW9BwHfRmx/D/QJve4DfB96/Rw2AXe9ch3tAfwTmzg8rq8FSAFWYNMqFgCuur9r2KWjp4Veu0LlJNZ1j7iG7FCAOBF4F5tBLF6vZSvQo86+uPodAzKALXV/rofiOuKqhU7D+U37xagurdHLGLMbIPQcTkIYF9cX+jN9IrCUOL2WUBfFSmAf8AGwCSg2xvhDRSLrWytnLhDOmdtR/AH4LyCcpr478XstBnhfRJaHchBD/P2ODQHygT+HusFeEJFUDsF1xFtAjyp3aRzr8NcnImnAW8AvjDElTRVtYF+HuRZjTMAYMwHbup0CjG6oWOi5w16LiJwF7DPGLI/c3UDRDn8tIccaYyZhuyF+LiLTmyjbUa/FBUwCnjXGTATKqeleaUibXUe8BfRo8pvGg70i0gcg9LwvtL9DX5+IuLHB/FVjzNuh3XF5LWHGmGLgU+x9gUyxOXGhdn07cs7cY4FzRGQr8Dq22+UPxOe1YIzZFXreB7yD/bKNt9+xPCDPGLM0tP0mNsC3+3XEW0CPJr9pPIjMwXoltj86vP+nobveRwMHwn+ixZqICDbV4HpjzBMRh+LxWrJEJDP0Ohk4GXvT6hNsTlyofy0dMmeuMeZuY0y2MWYQ9v/Dx8aYy4jDaxGRVBFJD78GTgW+Jc5+x4wxe4AdIjIytOskYB2H4jpifQOhBTcczgA2YPs874l1faKo72vAbsCH/Sa+Bttn+RHwQ+i5W6isYEfxbALWADmxrn/EdfwI+2fgamBl6HFGnF7LkcA3oWv5Frg3tH8I8DWwEfg7kBjanxTa3hg6PiTW19DIdf0YeDderyVU51Whx9rw/+84/R2bAOSGfsf+AXQ9FNehU/+VUqqTiLcuF6WUUo3QgK6UUp2EBnSllOokNKArpVQnoQFdKaU6CQ3oSinVSWhAV0qpTuL/A7lceUyfZV7IAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check how the losses changed over the training.\n",
    "losses = pd.DataFrame(model.history.history)\n",
    "losses.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows the loss and validation loss initially falling together, but then there's a seperation and a return to a rising loss in the validation data. This is a tell tale sign of overfitting to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/600\n",
      "426/426 [==============================] - 0s 547us/sample - loss: 0.6486 - val_loss: 0.6312\n",
      "Epoch 2/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.6086 - val_loss: 0.5927\n",
      "Epoch 3/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.5697 - val_loss: 0.5506\n",
      "Epoch 4/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.5268 - val_loss: 0.5034\n",
      "Epoch 5/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.4806 - val_loss: 0.4544\n",
      "Epoch 6/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.4358 - val_loss: 0.4104\n",
      "Epoch 7/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.3918 - val_loss: 0.3648\n",
      "Epoch 8/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.3557 - val_loss: 0.3242\n",
      "Epoch 9/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.3171 - val_loss: 0.2905\n",
      "Epoch 10/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.2876 - val_loss: 0.2598\n",
      "Epoch 11/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.2634 - val_loss: 0.2365\n",
      "Epoch 12/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.2452 - val_loss: 0.2184\n",
      "Epoch 13/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.2267 - val_loss: 0.2063\n",
      "Epoch 14/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.2140 - val_loss: 0.1907\n",
      "Epoch 15/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.2029 - val_loss: 0.1831\n",
      "Epoch 16/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.1921 - val_loss: 0.1752\n",
      "Epoch 17/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.1836 - val_loss: 0.1670\n",
      "Epoch 18/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.1720 - val_loss: 0.1588\n",
      "Epoch 19/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1639 - val_loss: 0.1536\n",
      "Epoch 20/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1558 - val_loss: 0.1504\n",
      "Epoch 21/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1501 - val_loss: 0.1423\n",
      "Epoch 22/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.1446 - val_loss: 0.1404\n",
      "Epoch 23/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.1356 - val_loss: 0.1358\n",
      "Epoch 24/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1290 - val_loss: 0.1337\n",
      "Epoch 25/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1253 - val_loss: 0.1301\n",
      "Epoch 26/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1189 - val_loss: 0.1269\n",
      "Epoch 27/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.1184 - val_loss: 0.1294\n",
      "Epoch 28/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1150 - val_loss: 0.1211\n",
      "Epoch 29/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1060 - val_loss: 0.1237\n",
      "Epoch 30/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1033 - val_loss: 0.1178\n",
      "Epoch 31/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0993 - val_loss: 0.1217\n",
      "Epoch 32/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0963 - val_loss: 0.1167\n",
      "Epoch 33/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0928 - val_loss: 0.1164\n",
      "Epoch 34/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0925 - val_loss: 0.1131\n",
      "Epoch 35/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0886 - val_loss: 0.1160\n",
      "Epoch 36/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0882 - val_loss: 0.1144\n",
      "Epoch 37/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0926 - val_loss: 0.1116\n",
      "Epoch 38/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0826 - val_loss: 0.1121\n",
      "Epoch 39/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0810 - val_loss: 0.1131\n",
      "Epoch 40/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0789 - val_loss: 0.1104\n",
      "Epoch 41/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0763 - val_loss: 0.1129\n",
      "Epoch 42/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0750 - val_loss: 0.1094\n",
      "Epoch 43/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0747 - val_loss: 0.1105\n",
      "Epoch 44/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0721 - val_loss: 0.1132\n",
      "Epoch 45/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0714 - val_loss: 0.1094\n",
      "Epoch 46/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0704 - val_loss: 0.1090\n",
      "Epoch 47/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0694 - val_loss: 0.1115\n",
      "Epoch 48/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0689 - val_loss: 0.1085\n",
      "Epoch 49/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.0686 - val_loss: 0.1082\n",
      "Epoch 50/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0672 - val_loss: 0.1125\n",
      "Epoch 51/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0671 - val_loss: 0.1116\n",
      "Epoch 52/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0683 - val_loss: 0.1083\n",
      "Epoch 53/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0654 - val_loss: 0.1100\n",
      "Epoch 54/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0644 - val_loss: 0.1078\n",
      "Epoch 55/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.0624 - val_loss: 0.1110\n",
      "Epoch 56/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0631 - val_loss: 0.1089\n",
      "Epoch 57/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0622 - val_loss: 0.1072\n",
      "Epoch 58/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0607 - val_loss: 0.1106\n",
      "Epoch 59/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0604 - val_loss: 0.1086\n",
      "Epoch 60/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0604 - val_loss: 0.1116\n",
      "Epoch 61/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0622 - val_loss: 0.1101\n",
      "Epoch 62/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0587 - val_loss: 0.1079\n",
      "Epoch 63/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0603 - val_loss: 0.1103\n",
      "Epoch 64/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0604 - val_loss: 0.1099\n",
      "Epoch 65/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0618 - val_loss: 0.1069\n",
      "Epoch 66/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0600 - val_loss: 0.1123\n",
      "Epoch 67/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0570 - val_loss: 0.1063\n",
      "Epoch 68/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0571 - val_loss: 0.1097\n",
      "Epoch 69/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0571 - val_loss: 0.1064\n",
      "Epoch 70/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0552 - val_loss: 0.1103\n",
      "Epoch 71/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0553 - val_loss: 0.1089\n",
      "Epoch 72/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0549 - val_loss: 0.1071\n",
      "Epoch 73/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0546 - val_loss: 0.1158\n",
      "Epoch 74/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0558 - val_loss: 0.1066\n",
      "Epoch 75/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0551 - val_loss: 0.1086\n",
      "Epoch 76/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0546 - val_loss: 0.1090\n",
      "Epoch 77/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0572 - val_loss: 0.1064\n",
      "Epoch 78/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0598 - val_loss: 0.1095\n",
      "Epoch 79/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0561 - val_loss: 0.1105\n",
      "Epoch 80/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0542 - val_loss: 0.1129\n",
      "Epoch 81/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0523 - val_loss: 0.1066\n",
      "Epoch 82/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0531 - val_loss: 0.1137\n",
      "Epoch 83/600\n",
      "426/426 [==============================] - 0s 31us/sample - loss: 0.0544 - val_loss: 0.1078\n",
      "Epoch 84/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0528 - val_loss: 0.1127\n",
      "Epoch 85/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0548 - val_loss: 0.1073\n",
      "Epoch 86/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0588 - val_loss: 0.1106\n",
      "Epoch 87/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0644 - val_loss: 0.1105\n",
      "Epoch 88/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0509 - val_loss: 0.1051\n",
      "Epoch 89/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0527 - val_loss: 0.1097\n",
      "Epoch 90/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0534 - val_loss: 0.1099\n",
      "Epoch 91/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0499 - val_loss: 0.1098\n",
      "Epoch 92/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0509 - val_loss: 0.1078\n",
      "Epoch 93/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0504 - val_loss: 0.1120\n",
      "Epoch 94/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0497 - val_loss: 0.1101\n",
      "Epoch 95/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0492 - val_loss: 0.1114\n",
      "Epoch 96/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0515 - val_loss: 0.1078\n",
      "Epoch 97/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0490 - val_loss: 0.1112\n",
      "Epoch 98/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0490 - val_loss: 0.1111\n",
      "Epoch 99/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0494 - val_loss: 0.1077\n",
      "Epoch 100/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0522 - val_loss: 0.1210\n",
      "Epoch 101/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0520 - val_loss: 0.1064\n",
      "Epoch 102/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0563 - val_loss: 0.1197\n",
      "Epoch 103/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0544 - val_loss: 0.1078\n",
      "Epoch 104/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0485 - val_loss: 0.1124\n",
      "Epoch 105/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0470 - val_loss: 0.1100\n",
      "Epoch 106/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0467 - val_loss: 0.1155\n",
      "Epoch 107/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0468 - val_loss: 0.1099\n",
      "Epoch 108/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0461 - val_loss: 0.1113\n",
      "Epoch 109/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0472 - val_loss: 0.1168\n",
      "Epoch 110/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0456 - val_loss: 0.1102\n",
      "Epoch 111/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0474 - val_loss: 0.1098\n",
      "Epoch 112/600\n",
      "426/426 [==============================] - 0s 33us/sample - loss: 0.0565 - val_loss: 0.1154\n",
      "Epoch 113/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.0470 - val_loss: 0.1141\n",
      "Epoch 00113: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20b5b641c88>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-defining the model and making use of callbacks.\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(30,activation = 'relu'))\n",
    "model.add(Dense(15,activation = 'relu'))\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam')\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "early_stop = EarlyStopping(monitor = 'val_loss',mode='min',verbose=1,patience=25)\n",
    "\n",
    "model.fit(x=X_train,y=y_train,epochs=600,validation_data=(X_test,y_test),callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model ran again and attempted to run all 600 epochs. However with the callbacks we set the EarlyStopping to monitor validation loss and to keep it to minimum loss, patience was set to 25 so the model ran another 25 epochs after noticing the need to early stop. We will now re-check the loss effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20b5b61b648>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_losses = pd.DataFrame(model.history.history)\n",
    "model_losses.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot has improved quite a bit with no real increase to val_loss but there still is some seperation in results. We can now see the effects of using Dropouts in our model. This will allow us to randomly switch off neurons within the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 426 samples, validate on 143 samples\n",
      "Epoch 1/600\n",
      "426/426 [==============================] - 0s 728us/sample - loss: 0.7157 - val_loss: 0.6828\n",
      "Epoch 2/600\n",
      "426/426 [==============================] - 0s 47us/sample - loss: 0.6922 - val_loss: 0.6729\n",
      "Epoch 3/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.6849 - val_loss: 0.6648\n",
      "Epoch 4/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.6621 - val_loss: 0.6532\n",
      "Epoch 5/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.6633 - val_loss: 0.6423\n",
      "Epoch 6/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.6496 - val_loss: 0.6316\n",
      "Epoch 7/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.6274 - val_loss: 0.6179\n",
      "Epoch 8/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.6244 - val_loss: 0.6003\n",
      "Epoch 9/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.6065 - val_loss: 0.5831\n",
      "Epoch 10/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.5843 - val_loss: 0.5634\n",
      "Epoch 11/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.5653 - val_loss: 0.5386\n",
      "Epoch 12/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.5614 - val_loss: 0.5141\n",
      "Epoch 13/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.5308 - val_loss: 0.4847\n",
      "Epoch 14/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.5084 - val_loss: 0.4516\n",
      "Epoch 15/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.4915 - val_loss: 0.4296\n",
      "Epoch 16/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.4866 - val_loss: 0.4046\n",
      "Epoch 17/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.4540 - val_loss: 0.3807\n",
      "Epoch 18/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.4574 - val_loss: 0.3574\n",
      "Epoch 19/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.4224 - val_loss: 0.3366\n",
      "Epoch 20/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.4118 - val_loss: 0.3157\n",
      "Epoch 21/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.3857 - val_loss: 0.3024\n",
      "Epoch 22/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.3658 - val_loss: 0.2844\n",
      "Epoch 23/600\n",
      "426/426 [==============================] - 0s 45us/sample - loss: 0.3719 - val_loss: 0.2680\n",
      "Epoch 24/600\n",
      "426/426 [==============================] - 0s 45us/sample - loss: 0.3758 - val_loss: 0.2555\n",
      "Epoch 25/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.3578 - val_loss: 0.2455\n",
      "Epoch 26/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.3222 - val_loss: 0.2382\n",
      "Epoch 27/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.3164 - val_loss: 0.2266\n",
      "Epoch 28/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.3013 - val_loss: 0.2150\n",
      "Epoch 29/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.3168 - val_loss: 0.2053\n",
      "Epoch 30/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.3093 - val_loss: 0.1980\n",
      "Epoch 31/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.3115 - val_loss: 0.1973\n",
      "Epoch 32/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.3075 - val_loss: 0.1911\n",
      "Epoch 33/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.2621 - val_loss: 0.1802\n",
      "Epoch 34/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.3169 - val_loss: 0.1877\n",
      "Epoch 35/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.2789 - val_loss: 0.1765\n",
      "Epoch 36/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.2914 - val_loss: 0.1711\n",
      "Epoch 37/600\n",
      "426/426 [==============================] - 0s 42us/sample - loss: 0.2796 - val_loss: 0.1794\n",
      "Epoch 38/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.2492 - val_loss: 0.1617\n",
      "Epoch 39/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.2762 - val_loss: 0.1584\n",
      "Epoch 40/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.2623 - val_loss: 0.1597\n",
      "Epoch 41/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.2419 - val_loss: 0.1573\n",
      "Epoch 42/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.2360 - val_loss: 0.1442\n",
      "Epoch 43/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.2614 - val_loss: 0.1398\n",
      "Epoch 44/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.2353 - val_loss: 0.1427\n",
      "Epoch 45/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.2270 - val_loss: 0.1427\n",
      "Epoch 46/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.2231 - val_loss: 0.1329\n",
      "Epoch 47/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.2339 - val_loss: 0.1368\n",
      "Epoch 48/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.2127 - val_loss: 0.1298\n",
      "Epoch 49/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.2327 - val_loss: 0.1301\n",
      "Epoch 50/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.2343 - val_loss: 0.1287\n",
      "Epoch 51/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.1950 - val_loss: 0.1298\n",
      "Epoch 52/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1937 - val_loss: 0.1260\n",
      "Epoch 53/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.2051 - val_loss: 0.1203\n",
      "Epoch 54/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1999 - val_loss: 0.1307\n",
      "Epoch 55/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.2001 - val_loss: 0.1219\n",
      "Epoch 56/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.2028 - val_loss: 0.1221\n",
      "Epoch 57/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1987 - val_loss: 0.1148\n",
      "Epoch 58/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.2216 - val_loss: 0.1259\n",
      "Epoch 59/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1950 - val_loss: 0.1188\n",
      "Epoch 60/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.2004 - val_loss: 0.1123\n",
      "Epoch 61/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1980 - val_loss: 0.1202\n",
      "Epoch 62/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1864 - val_loss: 0.1174\n",
      "Epoch 63/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1811 - val_loss: 0.1042\n",
      "Epoch 64/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1877 - val_loss: 0.1091\n",
      "Epoch 65/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1768 - val_loss: 0.1006\n",
      "Epoch 66/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1705 - val_loss: 0.1096\n",
      "Epoch 67/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1777 - val_loss: 0.1079\n",
      "Epoch 68/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1787 - val_loss: 0.0997\n",
      "Epoch 69/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1664 - val_loss: 0.1037\n",
      "Epoch 70/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.2095 - val_loss: 0.1034\n",
      "Epoch 71/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1763 - val_loss: 0.1063\n",
      "Epoch 72/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1607 - val_loss: 0.1053\n",
      "Epoch 73/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1561 - val_loss: 0.1002\n",
      "Epoch 74/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1671 - val_loss: 0.1063\n",
      "Epoch 75/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1728 - val_loss: 0.0948\n",
      "Epoch 76/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.1489 - val_loss: 0.0920\n",
      "Epoch 77/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1647 - val_loss: 0.0899\n",
      "Epoch 78/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "426/426 [==============================] - 0s 40us/sample - loss: 0.1902 - val_loss: 0.1002\n",
      "Epoch 79/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1507 - val_loss: 0.0945\n",
      "Epoch 80/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1774 - val_loss: 0.0981\n",
      "Epoch 81/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1641 - val_loss: 0.0937\n",
      "Epoch 82/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1508 - val_loss: 0.0957\n",
      "Epoch 83/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1432 - val_loss: 0.0968\n",
      "Epoch 84/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1631 - val_loss: 0.0882\n",
      "Epoch 85/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1638 - val_loss: 0.0991\n",
      "Epoch 86/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1557 - val_loss: 0.0987\n",
      "Epoch 87/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1714 - val_loss: 0.0933\n",
      "Epoch 88/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.1512 - val_loss: 0.0835\n",
      "Epoch 89/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1214 - val_loss: 0.1051\n",
      "Epoch 90/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1433 - val_loss: 0.0867\n",
      "Epoch 91/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1450 - val_loss: 0.0854\n",
      "Epoch 92/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1443 - val_loss: 0.0931\n",
      "Epoch 93/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1388 - val_loss: 0.0834\n",
      "Epoch 94/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1360 - val_loss: 0.0916\n",
      "Epoch 95/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1349 - val_loss: 0.0820\n",
      "Epoch 96/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.1365 - val_loss: 0.0893\n",
      "Epoch 97/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1327 - val_loss: 0.0818\n",
      "Epoch 98/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1449 - val_loss: 0.0803\n",
      "Epoch 99/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1415 - val_loss: 0.0900\n",
      "Epoch 100/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1516 - val_loss: 0.0900\n",
      "Epoch 101/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1384 - val_loss: 0.0874\n",
      "Epoch 102/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1277 - val_loss: 0.0822\n",
      "Epoch 103/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1333 - val_loss: 0.0821\n",
      "Epoch 104/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1403 - val_loss: 0.0948\n",
      "Epoch 105/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1253 - val_loss: 0.0884\n",
      "Epoch 106/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1297 - val_loss: 0.0824\n",
      "Epoch 107/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1553 - val_loss: 0.0933\n",
      "Epoch 108/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1307 - val_loss: 0.0737\n",
      "Epoch 109/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1286 - val_loss: 0.0955\n",
      "Epoch 110/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1285 - val_loss: 0.0869\n",
      "Epoch 111/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1451 - val_loss: 0.1035\n",
      "Epoch 112/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1453 - val_loss: 0.0841\n",
      "Epoch 113/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1356 - val_loss: 0.0758\n",
      "Epoch 114/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1203 - val_loss: 0.0981\n",
      "Epoch 115/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1443 - val_loss: 0.0739\n",
      "Epoch 116/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1291 - val_loss: 0.1022\n",
      "Epoch 117/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1245 - val_loss: 0.0887\n",
      "Epoch 118/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1185 - val_loss: 0.0758\n",
      "Epoch 119/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1158 - val_loss: 0.0767\n",
      "Epoch 120/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1162 - val_loss: 0.0956\n",
      "Epoch 121/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1280 - val_loss: 0.0894\n",
      "Epoch 122/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1106 - val_loss: 0.0772\n",
      "Epoch 123/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1138 - val_loss: 0.0765\n",
      "Epoch 124/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.1110 - val_loss: 0.0906\n",
      "Epoch 125/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1010 - val_loss: 0.0739\n",
      "Epoch 126/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1143 - val_loss: 0.0817\n",
      "Epoch 127/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1287 - val_loss: 0.0834\n",
      "Epoch 128/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1139 - val_loss: 0.0973\n",
      "Epoch 129/600\n",
      "426/426 [==============================] - 0s 40us/sample - loss: 0.1225 - val_loss: 0.0938\n",
      "Epoch 130/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1200 - val_loss: 0.0906\n",
      "Epoch 131/600\n",
      "426/426 [==============================] - 0s 35us/sample - loss: 0.1143 - val_loss: 0.0856\n",
      "Epoch 132/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1259 - val_loss: 0.0828\n",
      "Epoch 133/600\n",
      "426/426 [==============================] - 0s 38us/sample - loss: 0.1212 - val_loss: 0.0744\n",
      "Epoch 00133: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x20b5bb1c7c8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-define the model again and include dropouts.\n",
    "model = Sequential()\n",
    "model.add(Dense(30,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(15,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(1,activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "model.fit(x=X_train,y=y_train,epochs=600,validation_data=(X_test,y_test),callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x20b5be6d5c8>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check the loss again.\n",
    "model_losses = pd.DataFrame(model.history.history)\n",
    "model_losses.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98        55\n",
      "           1       0.99      0.99      0.99        88\n",
      "\n",
      "    accuracy                           0.99       143\n",
      "   macro avg       0.99      0.99      0.99       143\n",
      "weighted avg       0.99      0.99      0.99       143\n",
      "\n",
      "\n",
      "[[54  1]\n",
      " [ 1 87]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model with test data.\n",
    "preds = model.predict_classes(X_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print( classification_report(y_test,preds))\n",
    "print()\n",
    "print( confusion_matrix(y_test,preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see some very good results here, 98% f1-score for benign and 99% f1-score for malignant classes. The confusion matrix shows 1 wrong prediction."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
